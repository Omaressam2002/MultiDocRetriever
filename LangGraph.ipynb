{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ba1890a5-cc70-441b-a202-58a51736177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember your name, it's Omar. \n",
      "\n",
      "As for your age, I can make a wild guess based on the fact that you're a fresh graduate. Typically, students graduate with a bachelor's degree around the age of 21-23. Considering that, I'm going to take a wild guess and say you might be around 22 years old. Please keep in mind that this is just a rough estimate and not an accurate prediction.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "import os\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "history = \"\"\"User: hii my name is Omar and i am Computer Engineering Fresh Graduate\n",
    "Assistant: Hello Omar! Congratulations on your recent graduation in Computer Engineering! That's a fantastic achievement. What are your plans now that you've completed your degree? Are you looking to pursue a master's degree, or are you interested in exploring job opportunities in the field of computer engineering? I'm here to help and offer any guidance or advice if you need it!\n",
    "\"\"\"\n",
    "\n",
    "question = \"do you remember my name ? and can you wildly guess my age through previous conversations?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the following question based on the previous conversation history. \n",
    "If the question asks about personal information like name or age, rely only on what was explicitly mentioned before.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Conversation so far:\n",
    "{history}\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0a52fde2-d899-4809-a3b0-2d1607456d10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "0a52fde2-d899-4809-a3b0-2d1607456d10",
    "outputId": "9f55b5ac-7469-4fe1-98cf-72250ce6e2bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import List\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import LLMResult, Generation\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "import os\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "# index for each type of data , dynamically add to the retriever database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a14a8ca-a639-4303-bb43-8d563edd5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = SentenceTransformer(\"models/e5-large-v2\")\n",
    "model = SentenceTransformer(\"models/e5-small-v2\")\n",
    "\n",
    "class E5Embedder:\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return model.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6eb24a6-32a1-43ed-a561-3a83113afc1b",
   "metadata": {
    "id": "ce64c95b-41cb-4aa6-9bf7-e917b574d2e0"
   },
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.6,\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\") \n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=PINECONE_ENV)\n",
    "INDEX_NAME = \"multi-doc-index\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b98b610-6711-43de-8ddc-ec1462b64096",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pdf_docs = [Document(\"HI THIS IS PREFLMR AGENT AGAIN AGAIN\"),Document(\n",
    "                    page_content=\"HI THIS IS PREFLMR AGENT AGAIN\",\n",
    "                    metadata={\n",
    "                        \"file_name\": \"agent_intro.txt\",\n",
    "                        \"source\": \"manual_upload\",\n",
    "                        \"timestamp\": \"2025-11-05\"\n",
    "                    }),\n",
    "                  Document(\n",
    "                    page_content=\"THIS IS NOT PREFLMR\",\n",
    "                    metadata={\n",
    "                        \"file_name\": \"agent_intro.txt\",\n",
    "                        \"source\": \"manual_upload\",\n",
    "                        \"timestamp\": \"2025-11-05\"\n",
    "                    }\n",
    "                )]\n",
    "vectorstore_pdf = PineconeVectorStore.from_documents(\n",
    "            split_pdf_docs,\n",
    "            embedding=E5Embedder(),\n",
    "            namespace=\"txt\",\n",
    "            index_name=INDEX_NAME\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3ac724c-a48d-40ed-9706-974fc7012707",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(\n",
    "                embedding=E5Embedder(),\n",
    "                index_name=INDEX_NAME,\n",
    "                namespace=\"txt\"\n",
    "            )         \n",
    "vectorstore.delete(\n",
    "    filter={\"file_name\": \"agent_intro.txt\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5102c6c-0cd1-4ac9-a93d-8905b80b7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INDEX_NAME not in [i.name for i in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67745de9-f5c8-4634-a09d-0ca259184345",
   "metadata": {
    "id": "ce64c95b-41cb-4aa6-9bf7-e917b574d2e0"
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "def clean(txt):\n",
    "    # apply data cleaning if needed\n",
    "    return txt\n",
    "\n",
    "def prefix_passage_texts(split_docs):\n",
    "    prefixed_docs = []\n",
    "    for doc in split_docs:\n",
    "        new_doc = Document(\n",
    "            page_content=\"passage: \" + clean(doc.page_content.strip()),\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        prefixed_docs.append(new_doc)\n",
    "    return prefixed_docs\n",
    "\n",
    "def ingestion_pipeline(doc_folder=\"./docs/\"):\n",
    "    # chunking based on type\n",
    "    pdf_paths = [x for x in os.listdir(doc_folder) if x[-4:] == \".pdf\"]\n",
    "    text_paths = [x for x in os.listdir(doc_folder) if x[-4:] == \".txt\"]\n",
    "\n",
    "    pdf_docs = []\n",
    "    for path in pdf_paths:\n",
    "        loader = PyPDFLoader(os.path.join(doc_folder,path))\n",
    "        pdf_docs.extend(loader.load())\n",
    "\n",
    "    # clean pdf_docs\n",
    "    if pdf_docs:\n",
    "        # chunking\n",
    "        split_pdf_docs = splitter.split_documents(pdf_docs)\n",
    "        \n",
    "        # cleaning\n",
    "        split_pdf_docs = prefix_passage_texts(split_pdf_docs)\n",
    "\n",
    "        \n",
    "        print(f\"Embedding {len(split_pdf_docs)} PDF chunks...\")\n",
    "\n",
    "        vectorstore_pdf = PineconeVectorStore.from_documents(\n",
    "            split_pdf_docs,\n",
    "            embedding=E5Embedder(),\n",
    "            namespace=\"pdf\",\n",
    "            index_name=INDEX_NAME\n",
    "        )\n",
    "\n",
    "            \n",
    "    # === Text ingestion ===\n",
    "    text_docs = []\n",
    "    for path in text_paths:\n",
    "        loader = TextLoader(os.path.join(doc_folder, path))\n",
    "        text_docs.extend(loader.load())\n",
    "\n",
    "    if text_docs:\n",
    "        #chunking\n",
    "        split_text_docs = splitter.split_documents(text_docs)\n",
    "\n",
    "        # cleaning\n",
    "        split_text_docs = pprefix_passage_texts(split_text_docs)\n",
    "        \n",
    "        print(f\"Embedding {len(split_text_docs)} Text chunks...\")\n",
    "        vectorstore_pdf = PineconeVectorStore.from_documents(\n",
    "            split_text_docs,\n",
    "            embedding=E5Embedder(),\n",
    "            namespace=\"txt\",\n",
    "            index_name=INDEX_NAME\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17b27b8-1e8b-40b8-9a93-2c0e23200902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 254 PDF chunks...\n"
     ]
    }
   ],
   "source": [
    "ingestion_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006cbac3-8dd9-467d-99cb-e6a3337938f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__module__', '__annotations__', '__doc__', '_index', '_async_index', '__init__', 'index', 'async_index', 'embeddings', 'add_texts', 'aadd_texts', '__aenter__', '__aexit__', 'aclose', '_async_index_context', 'similarity_search_by_vector', 'asimilarity_search_by_vector', 'similarity_search_with_score', 'asimilarity_search_with_score', 'similarity_search_by_vector_with_score', 'asimilarity_search_by_vector_with_score', 'similarity_search', 'asimilarity_search', '_select_relevance_score_fn', '_cosine_relevance_score_fn', 'max_marginal_relevance_search_by_vector', 'amax_marginal_relevance_search_by_vector', 'max_marginal_relevance_search', 'amax_marginal_relevance_search', 'get_pinecone_index', 'from_texts', 'afrom_texts', 'from_existing_index', 'delete', 'adelete', '__abstractmethods__', '_abc_impl'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PineconeVectorStore.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2122b31e-59e0-441d-bf19-995862113b32",
   "metadata": {
    "id": "2122b31e-59e0-441d-bf19-995862113b32"
   },
   "outputs": [],
   "source": [
    "# LangGraph nodes\n",
    "vectorstore_pdf = PineconeVectorStore.from_existing_index(\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=E5Embedder(),\n",
    "    namespace=\"pdf\"   \n",
    ")\n",
    "\n",
    "vectorstore_text = PineconeVectorStore.from_existing_index(\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=E5Embedder(),\n",
    "    namespace=\"text\"   \n",
    ")\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# i want the llm to decide if it has enough knowledge to answer the question before retrieving\n",
    "\n",
    "def retrieve_node(state):\n",
    "    query = \"query: \" + state[\"question\"]\n",
    "\n",
    "    # how does caching work with RAG\n",
    "    # get relevant docs top 10 from each index and then rerank them all and select top 5 from the 30\n",
    "    pdf_results = vectorstore_pdf.similarity_search(query, k=30)\n",
    "    # csv_results = vectorstore_csv.similarity_search(query, k=30)\n",
    "    text_results = vectorstore_text.similarity_search(query, k=30)\n",
    "    # rerank and choose top 10\n",
    "    all_results = pdf_results + text_results\n",
    "\n",
    "    # Build pairs for cross-encoder input\n",
    "    # make sure that csv is also a doc with page content\n",
    "    pairs = [(query, doc.page_content) for doc in all_results]\n",
    "\n",
    "    # Compute relevance scores\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    # Combine scores with docs\n",
    "    scored_docs = list(zip(all_results, scores))\n",
    "\n",
    "    # Sort by descending score\n",
    "    scored_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Take top 10\n",
    "    top_docs = [doc for doc, _ in scored_docs[:5]]\n",
    "\n",
    "    state[\"docs\"] = top_docs\n",
    "    return state\n",
    "\n",
    "def answer_node(state):\n",
    "    question = state[\"question\"]\n",
    "    # extract only passage\n",
    "    docs = [x for x in state[\"docs\"]]\n",
    "\n",
    "    # Build context from docs\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "    # Create a prompt for the LLM\n",
    "    prompt = f\"\"\"Answer the following question using the context below. and mention the exact reference or lines that you got your answer from.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    # Use your custom Groq LLM\n",
    "    response = llm([HumanMessage(content=prompt)])\n",
    "\n",
    "\n",
    "    state['answer'] = response\n",
    "    return state\n",
    "\n",
    "from typing import TypedDict, List\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    docs : List[str]\n",
    "    answer: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "024cec43-0b18-4adf-b2ff-b4fb615f8cc1",
   "metadata": {
    "id": "024cec43-0b18-4adf-b2ff-b4fb615f8cc1",
    "outputId": "017376a9-f3cc-4382-e612-86cc7766fdc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: content='The term \"token-based\" is not explicitly defined in the provided context. However, based on the usage of the term \"token\" and \"token embeddings\" throughout the passage, it can be inferred that \"token-based\" likely refers to a method or approach that utilizes individual tokens (e.g., words or subwords) and their corresponding embeddings.\\n\\nFor example, in the line: \"(5) the scores between queries and documents are computed based on late-interaction, allowing each query token to interact with all document token embeddings.\" (Reference: passage, line 5)\\n\\nThis line suggests that the approach is based on the interaction between individual tokens (query tokens and document tokens) and their embeddings.\\n\\nAnother relevant line is: \"All token-level embeddings are concatenated to form the query representation Q.\" (Reference: passage, line 1 of the second passage)\\n\\nThis line implies that the approach involves working with token-level embeddings, which are then combined to form a query representation.\\n\\nWhile the term \"token-based\" is not explicitly defined, these lines provide context for understanding what it might entail in this specific context.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 223, 'prompt_tokens': 806, 'total_tokens': 1029, 'completion_time': 0.604067876, 'prompt_time': 0.069114892, 'queue_time': 0.08491313, 'total_time': 0.673182768}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None} id='run--c45eeba4-2e0f-484b-bef4-8c270e1e5c5e-0' usage_metadata={'input_tokens': 806, 'output_tokens': 223, 'total_tokens': 1029}\n"
     ]
    }
   ],
   "source": [
    "# Build LangGraph\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"retrieve\", retrieve_node)\n",
    "builder.add_node(\"answer\", answer_node)\n",
    "builder.set_entry_point(\"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"answer\")\n",
    "builder.set_finish_point(\"answer\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Run it\n",
    "result = graph.invoke({\"question\": \"what is token based?\"})\n",
    "print(\"Answer:\", result['answer'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f7a3cfc-685a-4fc9-9a7a-faa86899c8a4",
   "metadata": {
    "id": "0f7a3cfc-685a-4fc9-9a7a-faa86899c8a4",
    "outputId": "fe397a17-53d1-4b4b-9a0c-59848d7d5aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: PreFLMR is a strong multi-modal retriever pre-trained on a vision-language corpus of over ten million items. \n",
      "\n",
      "I got this answer from the following lines: \n",
      "\"• PreFLMR, a strong multi-modal retriever pre- trained on a vision-language corpus of over ten million items.\"\n",
      "\n",
      "Additionally, it is also described as \"a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA\" in the passage: \n",
      "\"We use M2KR to develop PreFLMR, a pre- trained version of the recently developed Fine- grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we re-\"\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"what is preflmr?\"})\n",
    "print(\"Answer:\", result['answer'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68842b6b-67bd-4bc5-a006-da011035c8b7",
   "metadata": {
    "id": "68842b6b-67bd-4bc5-a006-da011035c8b7",
    "outputId": "10d02b09-fa07-400d-e8b4-680cbf5a49b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Creating a Visual Question Answering (VQA) model using Graph Neural Networks (GNNs) involves several steps. Here's a detailed explanation of how to create a VQA model using GNNs, based on the context provided:\n",
      "\n",
      "**Step 1: Define the Problem and Dataset**\n",
      "The goal of VQA is to classify a given image-question pair into one of the possible answer classes. The dataset used for training and evaluation is crucial. In this context, the VCR (Visual Commonsense Reasoning) and GQA (Visual Question Answering) datasets are used.\n",
      "\n",
      "**Step 2: Construct the Multimodal Semantic Graph**\n",
      "The multimodal semantic graph is a crucial component of the VQA-GNN model. It represents the relationships between different objects, attributes, and concepts in the image and question. The graph consists of two main components:\n",
      "\n",
      "* **Scene Graph**: represents the visual relationships between objects in the image.\n",
      "* **Concept Graph**: represents the relationships between concepts and attributes in the question.\n",
      "\n",
      "The scene graph and concept graph are interconnected through a super node that represents the QA context.\n",
      "\n",
      "**Step 3: Define the GNN Architecture**\n",
      "The GNN architecture is used to perform inter-modal message passing between the scene graph and concept graph. The GNN module takes the node representations of the scene graph and concept graph as input and outputs updated node representations.\n",
      "\n",
      "The GNN module consists of multiple layers, each of which performs the following operations:\n",
      "\n",
      "* **Message Passing**: each node receives messages from its neighbors and updates its representation based on these messages.\n",
      "* **Aggregation**: the updated node representations are aggregated to form a new representation.\n",
      "\n",
      "**Step 4: Define the Node Representations**\n",
      "The node representations are critical in the GNN architecture. Each node in the scene graph and concept graph has a representation that captures its semantic meaning. The node representations are learned during training and are used as input to the GNN module.\n",
      "\n",
      "**Step 5: Define the QA-Context Node and QA-Concept Node**\n",
      "The QA-context node and QA-concept node are special nodes that represent the QA context and concept, respectively. These nodes are used to connect the scene graph and concept graph and to provide a unified representation of the image-question pair.\n",
      "\n",
      "**Step 6: Define the Output Layer**\n",
      "The output layer takes the final node representations from the GNN module and outputs a probability distribution over the possible answer classes. The output layer consists of a linear transformation followed by a softmax function.\n",
      "\n",
      "**Step 7: Train the Model**\n",
      "The VQA\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"explain to me in details how to create VQA model using GNNs\"})\n",
    "print(\"Answer:\", result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d089f-aca1-4a32-a046-e6b2f85b8997",
   "metadata": {
    "id": "607d089f-aca1-4a32-a046-e6b2f85b8997",
    "outputId": "c55dc6a1-4514-4fff-d31f-0e8addfe5e0a"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image,display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RR32fevVREPp",
   "metadata": {
    "id": "RR32fevVREPp"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save it\n",
    "vectorstore.save_local(\"db/\")\n",
    "\n",
    "# Later, load it\n",
    "new_store = FAISS.load_local(\"db/\", embeddings=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "244b7050-f410-4283-892c-06fc5e1da586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simplerllm\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/76/d3/d4ec875a3d0de4fdf6a1aae25e8baa35393ac3301862485aa0a31380712a/simplerllm-0.3.1.22-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.22-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting aiohttp>=3.9 (from simplerllm)\n",
      "  Obtaining dependency information for aiohttp>=3.9 from https://files.pythonhosted.org/packages/4e/b4/a0638ae1f12d09a0dc558870968a2f19a1eba1b10ad0a85ef142ddb40b50/aiohttp-3.13.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading aiohttp-3.13.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting cohere>=5.0 (from simplerllm)\n",
      "  Obtaining dependency information for cohere>=5.0 from https://files.pythonhosted.org/packages/69/5c/e312678fb4dff827c748980ec18918307d25e39ce006c84f7c6b32bc5641/cohere-5.20.0-py3-none-any.whl.metadata\n",
      "  Downloading cohere-5.20.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting duckduckgo_search>=5.3 (from simplerllm)\n",
      "  Obtaining dependency information for duckduckgo_search>=5.3 from https://files.pythonhosted.org/packages/db/72/c027b3b488b1010cf71670032fcf7e681d44b81829d484bb04e31a949a8d/duckduckgo_search-8.1.1-py3-none-any.whl.metadata\n",
      "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting lxml_html_clean>=0.1 (from simplerllm)\n",
      "  Obtaining dependency information for lxml_html_clean>=0.1 from https://files.pythonhosted.org/packages/10/4a/63a9540e3ca73709f4200564a737d63a4c8c9c4dd032bab8535f507c190a/lxml_html_clean-0.4.3-py3-none-any.whl.metadata\n",
      "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting newspaper3k>=0.2 (from simplerllm)\n",
      "  Obtaining dependency information for newspaper3k>=0.2 from https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl.metadata\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.26 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from simplerllm) (1.26.4)\n",
      "Collecting openai>=1.59 (from simplerllm)\n",
      "  Obtaining dependency information for openai>=1.59 from https://files.pythonhosted.org/packages/15/0e/331df43df633e6105ff9cf45e0ce57762bd126a45ac16b25a43f6738d8a2/openai-2.6.1-py3-none-any.whl.metadata\n",
      "  Using cached openai-2.6.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from simplerllm) (2.11.7)\n",
      "Collecting PyPDF2>=3.0 (from simplerllm)\n",
      "  Obtaining dependency information for PyPDF2>=3.0 from https://files.pythonhosted.org/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dotenv>=1.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from simplerllm) (1.1.1)\n",
      "Collecting python_docx>=1.1 (from simplerllm)\n",
      "  Obtaining dependency information for python_docx>=1.1 from https://files.pythonhosted.org/packages/d0/00/1e03a4989fa5795da308cd774f05b704ace555a70f9bf9d3be057b680bcf/python_docx-1.2.0-py3-none-any.whl.metadata\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: requests>=2.31 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from simplerllm) (2.32.5)\n",
      "Requirement already satisfied: youtube_transcript_api>=0.6 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from simplerllm) (1.2.2)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from simplerllm) (0.4.6)\n",
      "INFO: pip is looking at multiple versions of simplerllm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting simplerllm\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/99/e8/e8c5a39e1c53abbca53ccdd8c281872a08b9cb515c79e5fa8c8c97c9cd9a/simplerllm-0.3.1.20-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.20-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/b8/25/699024b9bea7f3e26ac2a61b1e0cb153fb65b111c8a79624385c41ceeee3/simplerllm-0.3.1.19-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.19-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/21/4f/7141ef973a5957dad9af589fffb2f069d6ae1cec7e3db9cda7a0f9eb7538/simplerllm-0.3.1.18-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.18-py3-none-any.whl.metadata (17 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/eb/9a/c501c1aba723dbac4aa9b0ed16a164ca50751357949738f256e1b2bbca4a/simplerllm-0.3.1.17-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.17-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/9e/74/447a2153715c754e1a0dcc06a5130fe21cca2e76caad49b3ebc9c7834f25/simplerllm-0.3.1.16-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.16-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/42/f4/9773388ba1fd597ade6dbcafc65ff876f7113cbedc1308303f20aeca4169/simplerllm-0.3.1.14-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.14-py3-none-any.whl.metadata (8.8 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/b5/38/31416edb7a328224ea9e178e296db6214e8c7b66541b765b4efb5acc8b78/simplerllm-0.3.1.13-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.13-py3-none-any.whl.metadata (8.8 kB)\n",
      "INFO: pip is still looking at multiple versions of simplerllm to determine which version is compatible with other requirements. This could take a while.\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/31/fe/1ded00b1d5d50801a838cce8e314ba9113f5644b53702bd368b4de2983b4/simplerllm-0.3.1.12-py3-none-any.whl.metadata\n",
      "  Downloading simplerllm-0.3.1.12-py3-none-any.whl.metadata (8.8 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/ec/b2/aaa42679bb587e44e106a7451c4711e62d60ec0888290e350651035ba04c/SimplerLLM-0.3.1.11-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.11-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting aiohttp==3.9.4 (from simplerllm)\n",
      "  Obtaining dependency information for aiohttp==3.9.4 from https://files.pythonhosted.org/packages/2a/7a/7df548c13bc37c6f7f72b9dd8cd64be329fb2e61666efcc8598e5cc84d86/aiohttp-3.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading aiohttp-3.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting duckduckgo-search==5.3.0 (from simplerllm)\n",
      "  Obtaining dependency information for duckduckgo-search==5.3.0 from https://files.pythonhosted.org/packages/01/0c/0d5bc90708f19dcc344dfa5afb3a2b612845fd9ce5d68f7a2a18a21ac46e/duckduckgo_search-5.3.0-py3-none-any.whl.metadata\n",
      "  Downloading duckduckgo_search-5.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting lxml-html-clean==0.1.1 (from simplerllm)\n",
      "  Obtaining dependency information for lxml-html-clean==0.1.1 from https://files.pythonhosted.org/packages/24/8e/56abd5586e3dd1f39bb9f876b8d8aa594f37d01c554081697184b1db6175/lxml_html_clean-0.1.1-py3-none-any.whl.metadata\n",
      "  Downloading lxml_html_clean-0.1.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting openai==1.59.8 (from simplerllm)\n",
      "  Obtaining dependency information for openai==1.59.8 from https://files.pythonhosted.org/packages/8c/cf/5b235e12ead3cd2098f9792776c966994c1bc558cba5799e12f3045227df/openai-1.59.8-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.59.8-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting pydantic==2.10.5 (from simplerllm)\n",
      "  Obtaining dependency information for pydantic==2.10.5 from https://files.pythonhosted.org/packages/58/26/82663c79010b28eddf29dcdd0ea723439535fa917fce5905885c0e9ba562/pydantic-2.10.5-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting python-dotenv==1.0.1 (from simplerllm)\n",
      "  Obtaining dependency information for python-dotenv==1.0.1 from https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting python-docx==1.1.0 (from simplerllm)\n",
      "  Obtaining dependency information for python-docx==1.1.0 from https://files.pythonhosted.org/packages/5f/d8/6948f7ac00edf74bfa52b3c5e3073df20284bec1db466d13e668fe991707/python_docx-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting Requests==2.31.0 (from simplerllm)\n",
      "  Obtaining dependency information for Requests==2.31.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting youtube-transcript-api==0.6.2 (from simplerllm)\n",
      "  Obtaining dependency information for youtube-transcript-api==0.6.2 from https://files.pythonhosted.org/packages/52/42/5f57d37d56bdb09722f226ed81cc1bec63942da745aa27266b16b0e16a5d/youtube_transcript_api-0.6.2-py3-none-any.whl.metadata\n",
      "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting simplerllm\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/70/54/525ebba231d0bb4042eb2bd025aedb16b1c2a83b59045a8f5415d4941c81/SimplerLLM-0.3.1.10-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.10-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/fd/2c/546b42eca390bf829dd8060f6ef83587820637c905fad7c0d18c48382c92/SimplerLLM-0.3.1.9-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.9-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/48/c5/567ee2160111b290c62c5306db09dcb092892f3e69f1d75a2b8e4cce7638/SimplerLLM-0.3.1.8-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.8-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting pandas==2.2.3 (from simplerllm)\n",
      "  Obtaining dependency information for pandas==2.2.3 from https://files.pythonhosted.org/packages/9c/b9/5cead4f63b6d31bdefeb21a679bc5a7f4aaf262ca7e07e2bc1c341b68470/pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m910.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting simplerllm\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/24/8e/c978d3a4f69067fecf1e1b510339c815a59db7b2a933a81ae2fb9c6904b4/SimplerLLM-0.3.1.7-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.7-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting pandas==2.2.2 (from simplerllm)\n",
      "  Obtaining dependency information for pandas==2.2.2 from https://files.pythonhosted.org/packages/96/08/9ad65176f854fd5eb806a27da6e8b6c12d5ddae7ef3bd80d8b3009099333/pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting simplerllm\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/c5/e0/842605b679e45ab4d48a60e7f6c298f58a932629555c674ee28f8303b2d7/SimplerLLM-0.3.1.6-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.6-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/97/5a/ce4045c19947d594d391f43ef36c124f4a9b6ed4605140c2ea55dcdce330/SimplerLLM-0.3.1.5-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.5-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/8b/61/e1300dc7c5c579e0cb4ebd5fae72fe843e08e9ea17bee9ea672de5e59dfa/SimplerLLM-0.3.1.3-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.3-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/34/a6/6df291bc0b8f3feb3b15a1f936788c3d9c7c37564d0e1c439bd31a7da718/SimplerLLM-0.3.1.1-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting openai==1.25.0 (from simplerllm)\n",
      "  Obtaining dependency information for openai==1.25.0 from https://files.pythonhosted.org/packages/32/54/e50ba99d35dd951f5ca94c54cb7fe2f492c8a3a87e5979e21194cccd1977/openai-1.25.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.25.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pydantic==2.7.1 (from simplerllm)\n",
      "  Obtaining dependency information for pydantic==2.7.1 from https://files.pythonhosted.org/packages/ed/76/9a17032880ed27f2dbd490c77a3431cbc80f47ba81534131de3c2846e736/pydantic-2.7.1-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytube==15.0.0 (from simplerllm)\n",
      "  Obtaining dependency information for pytube==15.0.0 from https://files.pythonhosted.org/packages/51/64/bcf8632ed2b7a36bbf84a0544885ffa1d0b4bcf25cc0903dba66ec5fdad9/pytube-15.0.0-py3-none-any.whl.metadata\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting simplerllm\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/09/1b/9e8bdbdb9f634c3a54e3232705f6712660c34a874a39fb966edb91a6f708/SimplerLLM-0.3.1.0-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/f3/4e/5a2d5be65b627fd1abd0cae2996a904c0ff8056433056c8c9d6a29cfdd8c/SimplerLLM-0.3.0.4-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.0.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/a4/b9/67df695ef1512f8e089779d89ff25be828946f5015b918ac38bf5543b4d7/SimplerLLM-0.3.0.3-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/2f/5f/9386bbc15df59659c5ca760d2bb9b708042a730efa9f534db31ce6f1c7f8/SimplerLLM-0.3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/b2/d0/304a4a7a5b65f68047453b29713190344c7d6c4d7cd218d1577ba37e04d4/SimplerLLM-0.3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Obtaining dependency information for simplerllm from https://files.pythonhosted.org/packages/65/52/0124613c2dfe9363ffe870ee93372f156d44ab53201fb597093dd2a3d171/SimplerLLM-0.3.0-py3-none-any.whl.metadata\n",
      "  Downloading SimplerLLM-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from aiohttp==3.9.4->simplerllm) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from aiohttp==3.9.4->simplerllm) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from aiohttp==3.9.4->simplerllm) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from aiohttp==3.9.4->simplerllm) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from aiohttp==3.9.4->simplerllm) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from aiohttp==3.9.4->simplerllm) (4.0.2)\n",
      "Requirement already satisfied: click>=8.1.7 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from duckduckgo-search==5.3.0->simplerllm) (8.1.7)\n",
      "Collecting curl-cffi>=0.6.2 (from duckduckgo-search==5.3.0->simplerllm)\n",
      "  Obtaining dependency information for curl-cffi>=0.6.2 from https://files.pythonhosted.org/packages/2c/1c/cdb4fb2d16a0e9de068e0e5bc02094e105ce58a687ff30b4c6f88e25a057/curl_cffi-0.13.0-cp39-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading curl_cffi-0.13.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson>=3.10.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from duckduckgo-search==5.3.0->simplerllm) (3.10.18)\n",
      "Requirement already satisfied: lxml in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from lxml-html-clean==0.1.1->simplerllm) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from newspaper3k>=0.2->simplerllm) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from newspaper3k>=0.2->simplerllm) (11.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from newspaper3k>=0.2->simplerllm) (6.0)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k>=0.2->simplerllm)\n",
      "  Obtaining dependency information for cssselect>=0.9.2 from https://files.pythonhosted.org/packages/ee/58/257350f7db99b4ae12b614a36256d9cc870d71d9e451e79c2dc3b23d7c3c/cssselect-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from newspaper3k>=0.2->simplerllm) (3.8.1)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k>=0.2->simplerllm)\n",
      "  Obtaining dependency information for feedparser>=5.2.1 from https://files.pythonhosted.org/packages/4e/eb/c96d64137e29ae17d83ad2552470bafe3a7a915e85434d9942077d7fd011/feedparser-6.0.12-py3-none-any.whl.metadata\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k>=0.2->simplerllm)\n",
      "  Obtaining dependency information for tldextract>=2.0.1 from https://files.pythonhosted.org/packages/67/7c/ea488ef48f2f544566947ced88541bc45fae9e0e422b2edbf165ee07da99/tldextract-5.3.0-py3-none-any.whl.metadata\n",
      "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k>=0.2->simplerllm)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k>=0.2->simplerllm)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from newspaper3k>=0.2->simplerllm) (2.8.2)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k>=0.2->simplerllm)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from openai==1.25.0->simplerllm) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from openai==1.25.0->simplerllm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from openai==1.25.0->simplerllm) (0.27.2)\n",
      "Requirement already satisfied: sniffio in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from openai==1.25.0->simplerllm) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from openai==1.25.0->simplerllm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from openai==1.25.0->simplerllm) (4.14.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from pandas==2.2.2->simplerllm) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from pandas==2.2.2->simplerllm) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from pydantic==2.7.1->simplerllm) (0.6.0)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic==2.7.1->simplerllm)\n",
      "  Obtaining dependency information for pydantic-core==2.18.2 from https://files.pythonhosted.org/packages/8c/91/290e2ec9fc2c870c1c0f97772ed97dfd784b5b0d04632fa9d390a5fb6562/pydantic_core-2.18.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pydantic_core-2.18.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from Requests==2.31.0->simplerllm) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from Requests==2.31.0->simplerllm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from Requests==2.31.0->simplerllm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from Requests==2.31.0->simplerllm) (2024.8.30)\n",
      "Requirement already satisfied: exceptiongroup in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai==1.25.0->simplerllm) (1.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->newspaper3k>=0.2->simplerllm) (2.3.2.post1)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from curl-cffi>=0.6.2->duckduckgo-search==5.3.0->simplerllm) (2.0.0)\n",
      "Requirement already satisfied: six in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from feedfinder2>=0.0.4->newspaper3k>=0.2->simplerllm) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k>=0.2->simplerllm)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httpcore==1.* in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai==1.25.0->simplerllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.25.0->simplerllm) (0.16.0)\n",
      "Requirement already satisfied: joblib in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k>=0.2->simplerllm) (1.3.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from nltk>=3.2.1->newspaper3k>=0.2->simplerllm) (2024.4.28)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k>=0.2->simplerllm)\n",
      "  Obtaining dependency information for requests-file>=1.4 from https://files.pythonhosted.org/packages/e1/d5/de8f089119205a09da657ed4784c584ede8381a0ce6821212a6d4ca47054/requests_file-3.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from tldextract>=2.0.1->newspaper3k>=0.2->simplerllm) (3.12.2)\n",
      "Requirement already satisfied: pycparser in /Users/omarelshobky/pytorch-test/env/lib/python3.9/site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo-search==5.3.0->simplerllm) (2.21)\n",
      "Downloading SimplerLLM-0.3.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.5/390.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading duckduckgo_search-5.3.0-py3-none-any.whl (21 kB)\n",
      "Downloading lxml_html_clean-0.1.1-py3-none-any.whl (11 kB)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.25.0-py3-none-any.whl (312 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m532.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m572.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m683.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m889.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m627.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
      "Downloading pydantic_core-2.18.2-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m685.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading curl_cffi-0.13.0-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m820.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m478.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m752.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=9643b1a4db9ce26fa6e10bf74794d6086d544bf8e3c2f41a72af82099ce95b3d\n",
      "  Stored in directory: /Users/omarelshobky/Library/Caches/pip/wheels/94/ad/df/a2a01300cea47d5695f242f7e925a805970106fd9e4b151468\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3339 sha256=94ad277d44f8025eb0c71afd38c9d7b94385781310d8cfeddf544485b96b48fe\n",
      "  Stored in directory: /Users/omarelshobky/Library/Caches/pip/wheels/43/4a/c2/61a371b2524ac90805391c660d8dc4505705297f25e2b85a5d\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=fa1ba0a536df32be2b862a1a68544de58b68c128e03cb0777714623fbc56af13\n",
      "  Stored in directory: /Users/omarelshobky/Library/Caches/pip/wheels/c2/22/59/8214a8d6357e9f540ce1f37f9a4362b6156b4ca81b37f1945f\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=dd18b89acbe06e5c85e5b8a9f691f8326a8c674fa8add6269aed13ff383270c4\n",
      "  Stored in directory: /Users/omarelshobky/Library/Caches/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, Requests, pytube, python-dotenv, python-docx, PyPDF2, pydantic-core, lxml-html-clean, feedparser, cssselect, youtube-transcript-api, requests-file, pydantic, pandas, feedfinder2, curl-cffi, aiohttp, tldextract, openai, duckduckgo-search, newspaper3k, simplerllm\n",
      "  Attempting uninstall: Requests\n",
      "    Found existing installation: requests 2.32.5\n",
      "    Uninstalling requests-2.32.5:\n",
      "      Successfully uninstalled requests-2.32.5\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 1.1.1\n",
      "    Uninstalling python-dotenv-1.1.1:\n",
      "      Successfully uninstalled python-dotenv-1.1.1\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.2\n",
      "    Uninstalling pydantic_core-2.33.2:\n",
      "      Successfully uninstalled pydantic_core-2.33.2\n",
      "  Attempting uninstall: youtube-transcript-api\n",
      "    Found existing installation: youtube-transcript-api 1.2.2\n",
      "    Uninstalling youtube-transcript-api-1.2.2:\n",
      "      Successfully uninstalled youtube-transcript-api-1.2.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.7\n",
      "    Uninstalling pydantic-2.11.7:\n",
      "      Successfully uninstalled pydantic-2.11.7\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.5\n",
      "    Uninstalling aiohttp-3.8.5:\n",
      "      Successfully uninstalled aiohttp-3.8.5\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.27 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.7.1 which is incompatible.\n",
      "langchain-community 0.3.30 requires requests<3.0.0,>=2.32.5, but you have requests 2.31.0 which is incompatible.\n",
      "langchain-core 0.3.78 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.7.1 which is incompatible.\n",
      "langgraph 0.6.8 requires pydantic>=2.7.4, but you have pydantic 2.7.1 which is incompatible.\n",
      "pinecone-plugin-assistant 1.8.0 requires requests<3.0.0,>=2.32.3, but you have requests 2.31.0 which is incompatible.\n",
      "tensorflow-macos 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.5.3 which is incompatible.\n",
      "tensorflow-macos 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n",
      "unstructured-client 0.42.3 requires pydantic>=2.11.2, but you have pydantic 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyPDF2-3.0.1 Requests-2.31.0 aiohttp-3.9.4 cssselect-1.3.0 curl-cffi-0.13.0 duckduckgo-search-5.3.0 feedfinder2-0.0.4 feedparser-6.0.12 jieba3k-0.35.1 lxml-html-clean-0.1.1 newspaper3k-0.2.8 openai-1.25.0 pandas-2.2.2 pydantic-2.7.1 pydantic-core-2.18.2 python-docx-1.1.0 python-dotenv-1.0.1 pytube-15.0.0 requests-file-3.0.1 sgmllib3k-1.0.0 simplerllm-0.3.0 tinysegmenter-0.3 tldextract-5.3.0 youtube-transcript-api-0.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e70568c-b65b-4113-bc9c-cd47c2134d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from utils import * # add reload module\n",
    "from modules import E5Embedder, MsMarcoCrossEncoder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.schema import HumanMessage\n",
    "load_dotenv()\n",
    "\n",
    "# needs to be faster\n",
    "\n",
    "\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\") \n",
    "INDEX_NAME = \"multi-doc-index\"\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "reranker = MsMarcoCrossEncoder()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "        temperature=0.6,\n",
    "        api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    docs : List[str]\n",
    "    answer: List[str]\n",
    "    error: str\n",
    "    llm: ChatGroq\n",
    "    route: str\n",
    "\n",
    "def router_node(state):\n",
    "    try:\n",
    "        state['router'] = ChatGroq(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.0,\n",
    "            api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n",
    "        response = state['router'].invoke([HumanMessage(content=f\"if i asked you a question like this : {state['question']} and you have a knowledge database for retrieval, Answer  with 'yes' if you would retrieve information to answer the question or Answer with 'no' if you would not retrieve.\")])\n",
    "        if response.content.lower() == \"yes\":\n",
    "            state['route'] = 'retrieve'\n",
    "        elif response.content.lower() == \"no\":\n",
    "            state['route'] = 'answer'\n",
    "        else:\n",
    "            state['route'] = 'error'\n",
    "            write_log(f\"[ERROR IN ROUTER NODE], LLM ANSWER INVALID: {response.content}\", level=\"error\")\n",
    "    except Exception as e:\n",
    "        state[\"error\"] = True\n",
    "        write_log(\"[ERROR IN ROUTER NODE] :\", level=\"error\", exc=e)\n",
    "    return state\n",
    "\n",
    "def answer_node(state):\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=f\" {state['question']} \")])\n",
    "        state['answer'] = response\n",
    "        write_log(f\"ANSWERED WITH NO NEED TO RETRIEVE : {state['question']} : {state['answer'].content}\")\n",
    "    except Exception as e:\n",
    "        state[\"error\"] = True\n",
    "        write_log(\"[ERROR IN ANSWER NODE] :\", level=\"error\", exc=e)\n",
    "    return state\n",
    "        \n",
    "def retrieve_node(state):\n",
    "    try:\n",
    "        # ask llm given the question and the history can you answer this question confidently without external knowledge?\n",
    "        # if yes then return and set a flag else retrieve\n",
    "        query = \"query: \" + state[\"question\"]\n",
    "        vectorstore_pdf = PineconeVectorStore.from_existing_index(\n",
    "            index_name=INDEX_NAME,\n",
    "            embedding=E5Embedder(),\n",
    "            namespace=\"pdf\"   \n",
    "        )\n",
    "        \n",
    "        vectorstore_text = PineconeVectorStore.from_existing_index(\n",
    "            index_name=INDEX_NAME,\n",
    "            embedding=E5Embedder(),\n",
    "            namespace=\"text\"   \n",
    "        )\n",
    "    \n",
    "        pdf_results = vectorstore_pdf.similarity_search(query, k=10)\n",
    "        text_results = vectorstore_text.similarity_search(query, k=10)\n",
    "        # rerank and choose top 10\n",
    "        all_results = pdf_results + text_results\n",
    "    \n",
    "        pairs = [(query, doc.page_content) for doc in all_results]\n",
    "    \n",
    "        # Compute relevance scores\n",
    "        scores = reranker.model.predict(pairs)\n",
    "    \n",
    "        # Combine scores with docs\n",
    "        scored_docs = list(zip(all_results, scores))\n",
    "    \n",
    "        # Sort by descending score\n",
    "        scored_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        # Take top 10\n",
    "        top_docs = [doc for doc, _ in scored_docs[:7]]\n",
    "    \n",
    "        state[\"docs\"] = top_docs\n",
    "    except Exception as e:\n",
    "        state[\"error\"] = True\n",
    "        write_log(\"[ERROR IN RETRIEVE NODE] :\", level=\"error\", exc=e)\n",
    "    return state\n",
    "\n",
    "def generation_node(state):\n",
    "    try:\n",
    "        question = state[\"question\"]\n",
    "        # extract only passage\n",
    "        docs = [x for x in state[\"docs\"]]\n",
    "    \n",
    "        # Build context from docs\n",
    "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    \n",
    "        # Create a prompt for the LLM\n",
    "        prompt = f\"\"\"Answer the following question using the context below, and mention the exact lines of context that helped you answer the question.\n",
    "    \n",
    "        Question: {question}\n",
    "    \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        # Use your custom Groq LLM\n",
    "        response = llm([HumanMessage(content=prompt)])\n",
    "    \n",
    "        # given the answer and the reference is this answer valid?\n",
    "    \n",
    "    \n",
    "        state['answer'] = response\n",
    "    except Exception as e:\n",
    "        state[\"error\"] = True\n",
    "        write_log(\"[ERROR IN GENERATION NODE] :\", level=\"error\", exc=e)\n",
    "    return state    \n",
    "    \n",
    "\n",
    "class PDFAgent:\n",
    "    \n",
    "    def __init__(self): \n",
    "        if INDEX_NAME not in [i.name for i in pc.list_indexes()]:\n",
    "            pc.create_index(\n",
    "                name=INDEX_NAME,\n",
    "                dimension=384,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "            )\n",
    "        \n",
    "        builder = StateGraph(GraphState)\n",
    "\n",
    "        builder.add_node(\"router\", router_node)\n",
    "        builder.add_node(\"answer\", answer_node)\n",
    "        builder.add_node(\"retrieve\", retrieve_node)\n",
    "        builder.add_node(\"generation\", generation_node)\n",
    "        \n",
    "        builder.set_entry_point(\"router\")\n",
    "        builder.add_conditional_edges(\"router\",lambda state: state[\"route\"],\n",
    "            {\n",
    "                \"error\": END,\n",
    "                \"retrieve\" : \"retrieve\",\n",
    "                \"answer\": \"answer\"\n",
    "            }\n",
    "            )\n",
    "        builder.add_edge(\"answer\", END)\n",
    "        builder.add_conditional_edges(\"retrieve\", lambda state: \"error\" if state.get(\"error\") else \"next\",\n",
    "            {\n",
    "                \"error\": END,\n",
    "                \"next\": \"generation\"\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        builder.add_edge(\"generation\", END)\n",
    "        \n",
    "        self.graph = builder.compile()\n",
    "        self.splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    \n",
    "    def get_response(self,query):\n",
    "        result = self.graph.invoke({\"question\": query})\n",
    "        return result['answer'].content\n",
    "\n",
    "    # not fast\n",
    "    def ingest_file(self, file_content, file_type, filename):\n",
    "        try :\n",
    "            if file_type == 'pdf':\n",
    "                # chunking\n",
    "                split_pdf_docs = self.splitter.split_documents(file_content)\n",
    "                # add prefix for e5 embedder\n",
    "                split_pdf_docs = prefix_passage_texts(split_pdf_docs, filename)\n",
    "                write_log(f\"Started Embedding {len(split_pdf_docs)} PDF chunks...\")\n",
    "                vectorstore_pdf = PineconeVectorStore.from_documents(\n",
    "                    split_pdf_docs,\n",
    "                    embedding=E5Embedder(),\n",
    "                    namespace=\"pdf\",\n",
    "                    index_name=INDEX_NAME\n",
    "                )\n",
    "                write_log(f\"Successfully Embedded {len(split_pdf_docs)} PDF chunks\")\n",
    "            else :\n",
    "                # chunking\n",
    "                split_text_docs = self.splitter.split_documents(file_content)\n",
    "                # add prefix for e5 embedder\n",
    "                split_text_docs = prefix_passage_texts(split_text_docs, filename)\n",
    "                write_log(f\"Started Embedding {len(split_text_docs)} Text chunks...\")\n",
    "                vectorstore_text = PineconeVectorStore.from_documents(\n",
    "                    split_text_docs,\n",
    "                    embedding=E5Embedder(),\n",
    "                    namespace=\"txt\",\n",
    "                    index_name=INDEX_NAME\n",
    "                )\n",
    "                write_log(f\"Successfully Embedded {len(split_text_docs)} Text chunks\")\n",
    "        except Exception as e :\n",
    "            write_log(\"[ERROR IN FILE INGESTING] :\", level=\"error\", exc=e)\n",
    "            return False\n",
    "        return True            \n",
    "            \n",
    "    def delete_file(self, file_type, file_path):\n",
    "        try: \n",
    "            write_log(f\"Removing File: {file_path}...\")\n",
    "            \n",
    "            vectorstore = PineconeVectorStore(\n",
    "                embedding=E5Embedder(),\n",
    "                index_name=INDEX_NAME,\n",
    "                namespace=file_type\n",
    "            )\n",
    "            \n",
    "            vectorstore.delete(\n",
    "                filter={\"filename\": file_path},\n",
    "            )\n",
    "            write_log(f\"Removed File: {file_path} \")\n",
    "        except Exception as e :\n",
    "            write_log(\"[ERROR IN FILE DELETING] :\", level=\"error\", exc=e)\n",
    "            return False\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ebe3435-f06c-4a79-aea1-de485f56d58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(INDEX_NAME)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5d02264c-f3be-481f-8160-558d0751e409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Agent = PDFAgent()\n",
    "from IPython.display import Image,display\n",
    "display(Image(Agent.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f8e90c3-0e5c-44bc-aa60-2432f7b0e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/messages\n",
      "Attempt 2 failed: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/messages\n",
      "Attempt 3 failed: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/messages\n",
      "All retry attempts failed.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from SimplerLLM.language.llm import LLM, LLMProvider\n",
    "\n",
    "# Create LLM instance\n",
    "llm = LLM.create(provider=LLMProvider.ANTHROPIC, model_name=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "# Generate response\n",
    "response = llm.generate_response(prompt=\"Explain quantum computing in simple terms\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7dc3099-6834-4e37-adda-5f72d1b15c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAILLM' object has no attribute 'ReliableLLM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m secondary_llm \u001b[38;5;241m=\u001b[39m LLM\u001b[38;5;241m.\u001b[39mcreate(provider\u001b[38;5;241m=\u001b[39mLLMProvider\u001b[38;5;241m.\u001b[39mANTHROPIC, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-5-sonnet-20241022\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create reliable LLM with automatic failover\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m reliable_llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReliableLLM\u001b[49m(primary_llm, secondary_llm)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# If primary fails, automatically uses secondary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m reliable_llm\u001b[38;5;241m.\u001b[39mgenerate_response(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain machine learning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenAILLM' object has no attribute 'ReliableLLM'"
     ]
    }
   ],
   "source": [
    "from SimplerLLM.language.llm import LLM, LLMProvider\n",
    "import SimplerLLM.language.llm as llm\n",
    "\n",
    "# Create primary and secondary LLMs\n",
    "primary_llm = LLM.create(provider=LLMProvider.OPENAI, model_name=\"gpt-4o\")\n",
    "secondary_llm = LLM.create(provider=LLMProvider.ANTHROPIC, model_name=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "# Create reliable LLM with automatic failover\n",
    "reliable_llm = llm.ReliableLLM(primary_llm, secondary_llm)\n",
    "\n",
    "# If primary fails, automatically uses secondary\n",
    "response = reliable_llm.generate_response(prompt=\"Explain machine learning\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb461e0d-2519-43ca-831e-dd1d38f48e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextDocument(file_size=4987758, word_count=14094, character_count=93053, content='PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal\\nRetrievers\\n*Weizhe Lin, Jingbiao Mei, Jinghong Chen, Bill Byrne\\nDepartment of Engineering\\nUniversity of Cambridge\\nCambridge, United Kingdom CB2 1PZ\\n{wl356, jm2245, jc2124, wjb31}@cam.ac.uk\\nAbstract\\nLarge Multimodal Models (LMMs) excel in\\nnatural language and visual understanding\\nbut are challenged by exacting tasks such as\\nKnowledge-based Visual Question Answering\\n(KB-VQA) which involve the retrieval of rel-\\nevant information from document collections\\nto use in shaping answers to questions. We\\npresent an extensive training and evaluation\\nframework, M2KR, for KB-VQA. M2KR con-\\ntains a collection of vision and language tasks\\nwhich we have incorporated into a single suite\\nof benchmark tasks for training and evalu-\\nating general-purpose multi-modal retrievers.\\nWe use M2KR to develop PreFLMR, a pre-\\ntrained version of the recently developed Fine-\\ngrained Late-interaction Multi-modal Retriever\\n(FLMR) approach to KB-VQA, and we re-\\nport new state-of-the-art results across a range\\nof tasks. We also present investigations into\\nthe scaling behaviors of PreFLMR intended to\\nbe useful in future developments in general-\\npurpose multi-modal retrievers. The code,\\ndemo, dataset, and pre-trained checkpoints are\\navailable at https://preflmr.github.io/.\\n1 Introduction\\nKnowledge-based Visual Question Answering (KB-\\nVQA) systems generate answers to queries consist-\\ning of questions about given images. Correctly\\nanswering these questions requires accessing rel-\\nevant world knowledge as well as vision and lan-\\nguage understanding. Despite their demonstrated\\nabilities in vision and language, recent Large Multi-\\nmodal Models (LMMs) (Chen et al., 2023a; Driess\\net al., 2023; Liu et al., 2023a; Zhu et al., 2023;\\nOpenAI, 2023) have performed poorly in recent\\nchallenging KB-VQA tasks (Chen et al., 2023b;\\nMensink et al., 2023a). One promising approach to\\nimprove their KB-VQA performance is Retrieval-\\nAugmented Generation (RAG), in which answer\\n*Weizhe Lin, Jingbiao Mei, and Jinghong Chen equally\\ncontributed to this work.generation by LMMs is grounded in relevant docu-\\nments retrieved from a knowledge base.\\nThe best-performing document retrieval ap-\\nproach for KB-VQA to date is Fine-grained Late-\\ninteraction Multi-modal Retrieval (FLMR) (Lin\\net al., 2023b). FLMR uses multi-dimensional\\nembedding matrices to represent documents and\\nqueries and then efficiently computes their rele-\\nvance scores via late-interaction (Khattab and Za-\\nharia, 2020), thus capturing fine-grained relevance\\nat the token level rather than at the passage level,\\nas in Dense Passage Retrieval (DPR) (Karpukhin\\net al., 2020). As a late-interaction retriever, FLMR\\nsubstantially outperforms DPR on a range of KB-\\nVQA tasks, with only minor speed penalties. In all\\nof these methods, model and data size are impor-\\ntant considerations. There has been much work in\\nscaling up Large Language Models (LLMs) (Ka-\\nplan et al., 2020; Alabdulmohsin et al., 2022; Chen\\net al., 2023a) and text-based retrieval (Ni et al.,\\n2022), but the scaling properties of these vision\\nand language retrieval systems have not been stud-\\nied. We therefore investigate the following three\\naspects of FLMR in KB-VQA.\\n(1)Vision & Text Encoding : We investigate how\\nKB-VQA performance is affected by scaling the\\nsize and complexity of vision and text encoders.\\n(2)Pre-training : As originally formulated,\\nFLMR employs simple, lightly trained Multi-Layer\\nPerceptrons (MLP). We investigate whether gains\\ncan be had through more extensive model pre-\\ntraining.\\n(3)Task Diversity : We gather nine open-source\\nvision-language datasets into a suite of benchmark\\ntasks, M2KR, for assessing Multi-task Multi-modal\\nKnowledge Retrieval. M2KR encompasses Image-\\nto-Text, Question-to-Text, and Image&Question-\\nto-Text retrieval tasks, and also includes prompting\\ninstructions that can be provided to an LLM for\\neach of the component tasks. General purpose\\nmulti-modal retrieval models can be created byarXiv:2402.08327v2  [cs.CL]  5 Jun 2024training on the entirety of the M2KR training data\\nand these models can then be evaluated on any\\nor all of the included tasks. Models can further\\nbe fine-tuned for specific M2KR tasks using the\\ntask-specific tuning data included in the collection.\\nWe show that M2KR can be used in training\\nan FLMR-based RAG LLM for multi-task multi-\\nmodal retrieval. We refer to this model as Pre-\\nFLRM (for Pre-trained FLMR). PreFLMR can be\\nused directly in its pre-trained form for multi-task\\nmulti-modal retrieval. PreFLMR can also be fine-\\ntuned for specific task-specific performance. In\\nboth uses we find that PreFLMR gives us substan-\\ntial gains across the M2KR tasks.\\nContributions of this paper are:\\n•The M2KR task suite encompassing nine\\ndatasets and three types of retrieval tasks for\\ntraining and evaluating general-purpose vision-\\nlanguage retrievers. We create M2KR by re-\\npurposing various vision and language data\\nsets that might not be originally created for\\nknowledge-based visual question answering,\\nthus ensuring a rich and diverse collection.\\n•PreFLMR, a strong multi-modal retriever pre-\\ntrained on a vision-language corpus of over\\nten million items. We show that PreFLMR\\nperforms well across a range of knowledge re-\\ntrieval tasks when given the appropriate instruc-\\ntions. We will release PreFLMR upon publica-\\ntion.\\n•A study of the scaling behaviour of FLMR in\\nterms of its model parameters and training data.\\nTo our knowledge, this is the first systematic\\nstudy of scaling in late-interaction based vision-\\nlanguage retrievers and should provide empiri-\\ncal guidance for future work.\\n2 Related Work\\nDocument Retrieval. DPR has become a corner-\\nstone in knowledge-intensive tasks (Chen et al.,\\n2017; Izacard and Grave, 2021; Guu et al., 2020;\\nLee et al., 2019; Lewis et al., 2020) as well as\\nin KB-VQA tasks due to its fast and precise re-\\ntrieval capabilities (Karpukhin et al., 2020; Gui\\net al., 2021; Luo et al., 2021; Lin and Byrne, 2022;\\nWu and Mooney, 2022). Recent developments\\nin retrieval methods, particularly Late Interaction\\nmodels (Khattab and Zaharia, 2020; Santhanam\\net al., 2022b), have shown notable performance\\ngains over DPR, albeit with some efficiency trade-\\noffs (Lin et al., 2023a,b). In multi-modal retrieval,FILIP (Yao et al., 2022) used pre-trained late in-\\nteraction models for single-modal image-text re-\\ntrieval, while FLMR (Lin et al., 2023b) extended\\nthe approach to multi-modal retrieval for KB-VQA\\nwith finer-grained visual and text features. This pa-\\nper further extends FLMR and explores its scaling\\nproperties in multi-modal retrieval. Similar to our\\nM2KR benchmark, A concurrent work (Wei et al.,\\n2023) introduces M-Beir, which combines several\\nretrieval tasks and can be used to train and evaluate\\nuniversal multi-modal retrievers.\\nAnother line of relevant research is KB-VQA\\nretrieval involving Named Entities, where retrieved\\ndocuments must identify the person in the image.\\nFor example, on ViQuAE (Lerner et al., 2022),\\nLerner et al. (2023) trains the retriever with a multi-\\nmodal inverse cloze task, while Lerner et al. (2024)\\nshows that combining mono- and cross-modal re-\\ntrieval improves performance. Both use a weighted\\nsum of BERT (Devlin et al., 2019) and CLIP (Rad-\\nford et al., 2021) embeddings, while our work trains\\na single multi-modal late-interaction retriever.\\nKnowledge-based VQA Systems. Recent multi-\\nmodal systems have significantly improved in com-\\nplex tasks like OKVQA (Schwenk et al., 2022) that\\nrequire external knowledge sources (Narasimhan\\net al., 2018; Garderes et al., 2020; Li et al., 2020;\\nWu et al., 2022; Marino et al., 2021; Chen et al.,\\n2023d; Gao et al., 2022; Gui et al., 2021; Hu\\net al., 2023b; Rao et al., 2023). Systems like\\nKAT (Gui et al., 2021) and REVIVE (Lin et al.,\\n2022) used LLMs (e.g. GPT-3) for generating can-\\ndidate answers. Challenges remain in answering\\nmore knowledge-intensive questions (Chen et al.,\\n2023b; Mensink et al., 2023a), underscoring the\\nneed for robust document retrieval. Mensink et al.\\n(2023a) showed that even state-of-the-art LLMs\\nperform poorly on difficult KB-VQA questions,\\nwith an accuracy of under 20% when retrieval is\\nnot incorporated. RA-VQAv2 (Lin et al., 2023b)\\nand prior work (Lin and Byrne, 2022; Luo et al.,\\n2021; Qu et al., 2021; Gao et al., 2022; Hu et al.,\\n2023b; Mensink et al., 2023a) demonstrated strong\\nperformance in KB-VQA by using external knowl-\\nedge databases.\\nScaling Retrieval Systems. Previous work has ex-\\nplored scaling laws in language/vision systems (Ka-\\nplan et al., 2020; Alabdulmohsin et al., 2022), re-\\nvealing correlations between model performance,\\ncomputation, number of parameters, and dataset\\nsizes. In retrieval, Ni et al. (2022) and Hu et al.\\n(2023b) both observe improvements in DPR-likemodels with one-dimensional embeddings by in-\\ncreasing the size of language/vision encoders. This\\npaper reports similar scaling investigations in multi-\\nmodal late-interaction retrieval.\\n3 The M2KR Benchmark Suite\\nCurrent multi-modal retrievers are typically trained\\nand evaluated on a single dataset only. To properly\\nstudy general-purpose multi-modal retrievers, we\\nintroduce the Multi-task Multi-modal Knowledge\\nRetrieval (M2KR) benchmark suite. We convert\\nnine diverse datasets, originally designed for vi-\\nsion and language tasks such as image recognition,\\nimage captioning, and conversational interactions,\\ninto a uniform retrieval format. Details of the pre-\\nprocessing steps, data partition, and prompting in-\\nstructions are provided in Appendix A, but we note\\nhere that re-purposing these datasets into a single\\nconsistent collection for knowledge-based visual\\nquestion answering represents a non-trivial effort.\\nM2KR will be released with our models.\\n3.1 Tasks and Datasets\\nTable 1 shows the composition of M2KR. We pre-\\nprocess the datasets into a uniform format and\\nwrite several task-specific prompting instructions\\nfor each dataset. The M2KR benchmark contains\\nthree types of tasks:\\nImage to Text (I2T) retrieval. These tasks eval-\\nuate the ability of a retriever to find relevant docu-\\nments associated with an input image. Component\\ntasks are WIT (Srinivasan et al., 2021), IGLUE-\\nen (Bugliarello et al., 2022), KVQA (Shah et al.,\\n2019), and CC3M (Sharma et al., 2018). CC3M\\nis included in the M2KR training set to improve\\nscene understanding but not in the validation/test\\nset as the task concerns caption generation, not\\nretrieval. The IGLUE test set, which is a subset\\nof WIT and has an established benchmark, is in-\\ncluded to enable comparison with the literature.\\nThe KVQA task, initially designed as a KB-VQA\\ntask, has been re-purposed into an I2T task for our\\nmodelling purposes (Appendix A.1.3).\\nQuestion to Text (Q2T) retrieval. This task is\\nbased on MSMARCO (Bajaj et al., 2018) and is\\nincluded to assess whether multi-modal retrievers\\nretain their ability in text-only retrieval after any\\nretraining for images.\\nImage & Question to Text (IQ2T) retrieval.\\nThis is the most challenging task which requires#Examples #Passages\\nDatasets Train Val Test Train Val/Test\\nI2T Retrieval\\nWIT 2.8M 20,102 5,120 4.1M 40K\\nIGLUE - - 685 - 1K\\nKVQA 65K 13,365 5,120 16.3K 4,648\\nCC3M 595K - - 595K -\\nQ2T Retrieval\\nMSMARCO 400K 6,980 5,120 8.8M 200K\\nIQ2T Retrieval\\nOVEN 339K 20,000 5,120 10K 3,192\\nLLaV A 351K - 5,120 351K 6,006\\nOKVQA 9K 5,046 5,046 110K 110K\\nInfoseek 100K - 4,708 100K 100K\\nE-VQA 212K 9,852 3,750 50K 50K\\nTable 1: Datasets in M2KR Benchmark Suite.\\njoint understanding of questions and images for\\naccurate retrieval. It consists of these subtasks:\\nOVEN (Hu et al., 2023a), LLaV A (Liu et al.,\\n2023b), OKVQA (Schwenk et al., 2022), Infos-\\neek (Chen et al., 2023c) and E-VQA (Mensink\\net al., 2023b). We note in particular that we convert\\nLLaV A, a multi-modal conversation dataset, into a\\nmulti-modal retrieval task (Appendix A.3.1).\\nThe training/validation/test examples are down-\\nsampled from the respective sets of the original\\ndatasets. We take test examples from the origi-\\nnal validation sets for LLaV A and Infoseek since\\nLLaV A has no test sets and the test set annotation\\nof Infoseek has not been released. We limit the\\nmaximum test samples to 5,120 for each dataset\\nto allow faster performance tests on all 9 datasets.\\nData preprocessing and partitioning details are in\\nAppendix A. We further verified that there are no\\nidentical images between the training and test sets\\nby checking the MD5 of the images, thereby pre-\\nventing data contamination during training. We\\nuse the validation splits to select hyperparameters\\nfor the models which can be found in detail in Ap-\\npendix B.2\\n3.2 Evaluation\\nWe use Recall@K (R@K) , which measures whether\\nat least one of the target documents is in the top-\\nKretrieved entries, to evaluate retrieval perfor-\\nmance. Additionally, for the datasets Infoseek, E-\\nVQA, and OKVQA, we mainly employ Pseudo\\nRecall/PRecall@K (PR@K) for evaluation. This\\nmetric measures whether at least one of the top K\\ndocuments includes the target answer.1\\n1In practice, PR@K more accurately reflects actual re-\\ntrieval performance and exhibits a stronger correlation withWe use R@10 for WIT and MSMARCO, and\\nR@1 for LLaV A and IGLUE. Other datasets are\\nevaluated with R@5 or PR@5. As in Table 2, we\\nalso report the average rank ( A.R.) of each model\\nover all datasets to indicate multi-task retrieval per-\\nformance relative to other models in comparison;\\nlower is better.\\n3.3 Baselines\\nFor each dataset, we show the best published re-\\nsults in recent literature as baselines, if available\\n(Table 2). For datasets without previous results\\nsuch as LLaV A and OVEN, we use our replica-\\ntion of CLIP (Radford et al., 2021) and FLMR as\\nzero-shot baselines following Lin et al. (2022).\\n4 PreFLMR Architecture and Training\\nOur architecture generally follows that of\\nFLMR (Lin et al., 2023b) as shown in Fig. 1. Pre-\\nFLMR uses token embedding matrices QandDto\\nrepresent query and document, respectively. Given\\na query ¯qconsisting of texts qand an image I,\\nPreFLMR uses a language model FLto obtain em-\\nbeddings of all tokens in q, a vision model FV\\nto obtain embeddings of I, and a mapping struc-\\ntureFMto project image embeddings into the text\\nembedding space. All token-level embeddings are\\nconcatenated to form the query representation Q.\\nThe document matrix Dis obtained similarly with\\nthe language model FLbut without visual features.\\nThe relevance score r(¯q, d)is computed via late-\\ninteraction (Khattab and Zaharia, 2020) between\\nQandD, aggregating the maximum dot products\\nover all query tokens with respect to all document\\ntokens (Eq. 9). lQandlDdenote the total number\\nof tokens in query ¯qand document d, respectively.\\nr(¯q, d) =lQX\\ni=1lDmax\\nj=1QiD⊤\\nj (1)\\nPreFLMR improves over FLMR in the follow-\\ning aspects: (1) While FLMR only uses the [CLS]\\nembedding from ViT as the image representation,\\nin PreFLMR we additionally extract embeddings\\nof image patches from ViT’s penultimate layer to\\nobtain a detailed visual representation. (2) We intro-\\nduce Transformer blocks with cross-attention into\\nthe mapping structure to obtain query-aware visual\\nthe ultimate VQA performance. This is because document\\nannotations are frequently incomplete, and alternative doc-\\numents within the corpus can often provide answers to the\\nquestions.representation. The Transformer blocks take the\\nimage patch embeddings as input, and use cross-\\nattention to integrate the features of the text en-\\ncoder. This allows PreFLMR to attend to differ-\\nent aspects of the image under different queries.\\nThese Transformer blocks are placed in parallel\\nwith FLMR’s 2-layer MLP mapping structure. (3)\\nWe append task-specific instructions to the text\\nquery to distinguish between tasks. The list of\\ninstructions for each task can be found in Ap-\\npendix A. For each query, the instruction is ran-\\ndomly sampled from the corresponding instruction\\nlist. Instruction tokens are masked in computing\\nrelevance score. For Q2T retrieval training, we\\nfeed a blank image as PreFLMR’s image input. For\\nI2T retrieval training, we use instructions as text\\ninput to PreFLMR.\\nPreFLMR training and inference follow that of\\nFLMR. When training on data consisting of several\\ndatasets, we randomly shuffle the entire training\\ndata and only use in-batch negative examples from\\nthe same corpus. Post-training, all documents are\\nindexed through PLAID (Santhanam et al., 2022a)\\nfor efficient late-interaction retrieval. For detailed\\nevaluation of retrieval efficiency, we refer readers\\nto Lin et al. (2023b).\\nThe detailed formal expression of the entire\\nmodel can be found in Appendix B.4.\\n4.1 Training Procedures\\nPreFLMR’s pre-training involves four stages.\\nStage 0: Text Encoder Pre-training. We train\\nColBERT following Khattab and Zaharia (2020)\\non the MSMARCO dataset to obtain the initial\\ncheckpoint for PreFLMR’s text encoder FL. This\\nis a straightforward replication of ColBERT used\\nas an initial text encoder as was done in FLMR, but\\nalso allowing for size variations.\\nStage 1: Training the Mapping Structure. In\\nthis stage, we only train the mapping structure FM,\\nkeeping the language and vision models frozen.\\nThis approach is an extension of the FLMR method-\\nology, incorporating a larger dataset and an addi-\\ntional cross-attention mapping layer. The train-\\ning is performed on the IQ2T dataset (LLaV A,\\nOVEN), I2T datasets (WIT, CC3M, KVQA), and\\nQ2T dataset (MSMARCO). Our objective is to en-\\ncompass all three task types in M2KR without the\\nneed to optimize the data mixing ratio or manu-\\nally select datasets to achieve an effective map-\\nping structure. This strategy is inspired by pre-Query T ext Encoder\\xa0 \\xa0 \\xa0 \\xa0 \\xa0Document\\nEncoderMaxSim MaxSim MaxSim ......\\nCross Attention\\nQuery V ision Encoder\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0image feature from  \\n\\xa0the last layerpatch embeddings from\\xa0the penultimate layer\\nObtain documents that correspond to\\xa0 the\\ninquiry alongside the provided image: \\xa0\\nhow big can this plant become?instruction token  \\nfeatures\\n[CLS]Dwarf cornel is a rhizomatous\\nherbaceous perennial growing to\\n20cm (8 inches) tall...1 2 3 4Token-level embeddings\\nMax similarity operation\\nSummation\\nQuery EncoderMaxSim\\nInput Image5 Scoring\\nKnowledge  \\nBaseTransformer Mapping NetworkLinear Mapping Network\\n(MLP)(Transformer)Figure 1: PreFLMR Model Architecture. (1) the text query consists of an instruction and a question, which is\\nencoded by a text encoder; (2) at the output of the vision encoder, a mapping network consisting of Multi-Layer\\nPerceptrons (MLP) converts the ‘[CLS]’ token representations into the same embedding space as the text encoder;\\n(3) the transformer blocks take in the patch image embeddings from the penultimate layer of the vision encoder\\nand attend to the text features by cross-attention; (4) a text encoder encodes documents in the knowledge base; (5)\\nthe scores between queries and documents are computed based on late-interaction, allowing each query token to\\ninteract with all document token embeddings.\\nvious studies (Lin et al., 2023b; Zhu et al., 2023;\\nLiu et al., 2023b), which utilized relatively simple\\nmulti-modal tasks to develop image-to-text map-\\npings.\\nWe mask the late-interaction token embeddings\\nin query matrix Qthat are produced by the lan-\\nguage model (not the token embeddings at the in-\\nput embedding layer). This encourages the Trans-\\nformer cross-attention layer to integrate informa-\\ntion from its textual inputs and enables PreFLMR\\nto perform IQ2T, I2T, and Q2T retrieval when pro-\\nvided with the appropriate instructions for each\\ntask.\\nStage 2: Intermediate KB-VQA Pre-training.\\nWe tune the text encoder FLand the mapping\\nstructure FMon the E-VQA dataset, a large and\\nhigh quality KB-VQA dataset, to enhance Pre-\\nFLMR’s retrieval performance. Including an in-\\ntermediate pre-training stage to align the model\\nwith in-domain data has been well-explored in the\\nliterature (e.g., Google’s TAPAS (Eisenschlos et al.,\\n2020)). We opt for a straightforward procedure to\\ntrain on E-VQA in the intermediate stage because\\nof its diversity, increased difficulty, and larger quan-\\ntity compared to other KB-VQA datasets. Specifi-\\ncally, E-VQA requires recognition of less common\\nentities such as spotted hyenas and relies on more\\nspecialized domain knowledge such as American\\nlandmarks, making it good for retrieval training.\\nThis design choice is well-supported by experimen-\\ntal results (Table 2 #8 vs #5, #3 vs #2) and weprovide detailed analysis in Sec. 5.6.\\nStage 3: Full-scale Fine-tuning. We train on\\nthe entire M2KR corpora, including OKVQA and\\nInfoseek. This stage is straightforward multi-task\\nlearning. We tune the entire model except the vi-\\nsion encoder FV. We adjust the dataset proportions\\nto ensure balanced learning on these datasets of\\nvarying sizes (Appendix B.1). Additionally, we use\\nseparate text encoders to encode queries and doc-\\numents; their parameters were shared in previous\\nsteps.\\n4.2 Training Configurations\\nWe use the Adam optimizer (Kingma and Ba, 2015)\\nwith a fixed learning rate of 10−4for the mapping\\nstructures and 10−5for other parameters in all ex-\\nperiments. Training was run up to 300k, 220k,\\n12k, and 50k steps in the four stages, respectively.\\nFull training configurations (including the hyperpa-\\nrameters for downstream VQA fine-tuning) can be\\nfound in Appendix B.2.\\n5 Experiments and Results\\nIn this section we present results of scaling Pre-\\nFLMR components (Sec. 5.2, 5.4), analyze the ef-\\nfect of each training stage (Sec. 5.3, 5.6), and eval-\\nuate on the downstream KB-VQA tasks (Sec. 5.5).\\nWe summarize our findings in Sec. 5.7. Multi-task\\nperformance refers to PreFLMR results, i.e. Stages\\n0, 1, 2, and 3, without any single-task fine-tuning.5.1 Model Variants\\nWe experiment with a range of model configu-\\nrations. Model sizes range from BERT-Small\\n(28.8M), BERT-Medium (41.1M), BERT-Base\\n(110M) to BERT-large (340M). ColBERT text en-\\ncoders are denoted as \"[BERT size]-[pre-training\\nscheme]\". There are two ColBERT pre-training\\nschemes: “v1” (Khattab and Zaharia, 2020) and\\n“v2” (Santhanam et al., 2022b). “v2” yields a bet-\\nter performing model than “v1” as evaluated on\\nMSMARCO. We compare models initialized from\\n“v1” and “v2” checkpoints to investigate how the\\nperformance of the initial uni-modal text retriever\\naffects the final multi-modal vision-language re-\\ntriever. Except for “Base-v2”, all ColBERT vari-\\nants are trained using our replication of ColBERT\\nfollowing the “v1” pre-training scheme.2For the\\nvision encoders, we use the ViT variants: ViT-B\\n(88M), ViT-L (303M) (Radford et al., 2021), ViT-H\\n(631M) and ViT-G (1.84B) (Cherti et al., 2023).\\n5.2 PreFLMR Performance\\nThe best-performing PreFLMR model (ViT-G +\\nBase-v2) outperforms other variants on most of\\nM2KR benchmark (Table 2, #13). Without single-\\ntask fine-tuning, PreFLMR outperforms baseline\\nmodels optimized for the individual tasks on 7 out\\nof 9 datasets, showcasing its capability as a general\\nvisual-language retriever. We now analyze how\\neach PreFLMR component affects performance.\\nVision Encoder Scaling. Scaling ViT from ViT-\\nB (86M) to ViT-G (1.8B) while keeping the text\\nencoder fixed brings about substantial performance\\ngain across all tasks (Table 2 #2, #5, #12, #13),\\ne.g. 48.8 to 59.6 on Infoseek and 67.9 to 73.1 on\\nE-VQA. The gain is greater when upgrading ViT-B\\nto ViT-L with recall improvements of ∼10% on\\nWIT, KVQA, OVEN, and Infoseek, showing the\\nbenefit of using better vision encoders. In addi-\\ntion, Fig. 3 in the appendix illustrates performance\\ngains in scaling the vision encoder with a radar plot.\\nHowever, the performance plateaus when scaling\\nViT to H and G. This observation aligns with re-\\nsults reported in the literature. OpenCLIP (Cherti\\net al., 2023) and BLIP2 (Li et al., 2023) have re-\\nported marginal or no performance improvement\\nwhen scaling beyond ViT-L across several datasets.\\nA plausible explanation is that if the ViT model\\nis not pre-trained on domain-specific data, it may\\nstruggle to make fine distinctions.\\n2The training code of “v2” has not been released officially.Text Encoder Scaling. Scaling up the text en-\\ncoder from BERT-Small-v1 to Medium-v1 to Base-\\nv1 (Table 2 #9, #10, #4) yields substantial perfor-\\nmance gain (A.R. 8.3, 8.2, and 5.6). However,\\nwe find that further scaling to Large-v1 (#11) ad-\\nversely impacts the performance (A.R. decreased\\nto 6.6). We attribute this to overfitting and unstable\\ntraining for large models given the available data\\n(Appendix B.3). The results suggest that BERT-\\nBase (110M) is adequate for building a capable\\nvision-language retriever.\\nImproving Text Encoder. Compared to Pre-\\nFLMR models initialized from Base-v1, models\\ninitialized from Base-v2 have better multi-tasking\\nperformance indicated by better A.R. (Table 2 #1\\nvs #2 and #4 vs #5). The gain from improving\\nthe text encoder is more substantial when using\\nthe “ViT-L” vision model (-2.4 A.R.) compared\\nto using “ViT-B” (-0.8 A.R.), indicating that the\\ntext encoder is relatively weak as the vision model\\nimproves.\\n5.3 Performance of Each PreFLMR Stage\\nIn this section, we analyze intermediate perfor-\\nmance in the earlier stages of pre-training to better\\nunderstand the scaling behaviour of PreFLMR.\\nText Encoder Pre-training. We train “ColBERT-\\nv1” at different sizes and evaluate on the MS-\\nMARCO dataset. Table 3 shows larger model sizes\\nconsistently yield better text retrieval performance.\\nIn contrast to the multi-modal case, scaling up to\\n“Large-v1” does not destabilize training and leads\\nto better performance compared to “Base-v1”.\\nTraining the Mapping Structures. Table 4 de-\\ntails system performance after Stage 1 training, in\\nwhich only the vision-language mapping structure\\nis trained. Similar to Sec.5.2, scaling up the vi-\\nsion encoder improves performance across tasks.\\nPreFLMR exhibits strong zero-shot KB-VQA per-\\nformance at this preliminary stage (50.87 in Infos-\\neek, 42.44 in E-VQA, and 52.14 in OKVQA). After\\nStage 1, PreFLMR with ViT-G performs worse than\\nother variants on IGLUE, E-VQA and OKVQA.\\nHowever, it attains the best performance on these\\ndatasets after Stage 3. This suggests that tuning\\nthe mapping structure alone is not enough to fully\\nutilize larger vision models.\\nIntermediate Pre-training. Stage 1 improves\\nperformance on KB-VQA tasks (Table 2 #3 vs #2I2T Q2T IQ2T\\nModelVis. Text Total WIT IGLUE KVQA MM OVEN LLaV A Infoseek E-VQA OKVQA A.R.\\nEnc. Enc. Param. R@10 R@1 R@5 R@5 R@5 R@1 PR@5 PR@5 PR@5\\nCLIP 28.1 44.1 23.8 - 22.0 33.0 17.1 10.4 5.7\\nSOTA FLMR GIVL FLMR ColBERT FLMR FLMR FLMR Lens FLMR\\nRes. 23.8 30.8 31.9 86.9 40.5 56.4 47.1 62.5368.1\\nMulti-task Performance\\n1 PreFLMR B B-v1 207M 41.5 56.8 28.6 77.9 45.9 67.4 48.9 65.4 67.2 9.0\\n2 PreFLMR B B-v2 207M 41.7 57.3 28.6 79.5 46.3 67.2 48.8 67.9 66.1 8.2\\n3 w/o inter. B B-v2 207M 41.2 56.8 26.5 78.2 43.7 65.0 47.0 57.3 65.1 10.9\\n4 PreFLMR L B-v1 422M 58.2 69.8 40.6 72.1 59.3 69.3 57.4 70.7 67.9 5.6\\n5 PreFLMR L B-v2 422M 60.5 69.2 43.6 78.7 59.8 71.8 57.9 70.8 68.5 3.2\\n6 ViT trainable L B-v2 422M 18.7 1.5 0.8 76.7 5.6 54.6 36.7 57.2 58.9 12.3\\n7 w/o instruct. L B-v2 422M 13.3 10.5 38.2 75.2 52.1 62.1 49.1 71.3 65.7 9.2\\n8 w/o inter. L B-v2 422M 60.0 72.0 40.5 80.3 56.1 70.5 55.4 67.0 66.6 4.6\\n9 PreFLMR L S-v1 334M 54.2 66.3 37.9 73.6 53.9 66.0 52.6 66.8 65.3 8.3\\n10 PreFLMR L M-v1 348M 56.2 67.9 37.1 72.9 55.5 64.7 52.2 70.4 65.3 8.2\\n11 PreFLMR L L-v1 677M 49.9 62.8 40.0 72.8 58.8 69.3 59.4 58.2 68.6 6.6\\n12 PreFLMR H B-v2 750M 60.5 71.2 39.4 78.5 61.5 72.3 59.5 71.7 68.1 3.1\\n13 PreFLMR G B-v2 1.96B 61.5 71.5 42.1 78.6 63.4 72.4 59.6 73.1 68.6 1.6\\nFine-tuned PreFLMR for Specific Downstream Tasks\\n14 PreFLMR L B-v2 422M 68.5 70.8 60.3 71.4 67.3\\n15 PreFLMR H B-v2 750M 69.3 72.3 62.3 72.1 70.5\\n16 PreFLMR G B-v2 1.96B 69.3 73.1 62.1 73.7 70.9\\nTable 2: PreFLMR performance on all datasets. PR stands for Pseudo Recall. Best multi-task performance is in\\nbold and best downstream fine-tuning performance is underlined. For the vision encoder, we compare ViT-B (B),\\nViT-L (L), ViT-H (H) and ViT-G (G). For the text encoder, we compare Base-v1 (B-v1), Base-v2 (B-v2), Small-v1\\n(S-v1), Medium-v1 (M-v1), and Large-v1 (L-v1). A.R.: Average Rank against all other models on all tasks. For\\nbaselines, we show: GIVL (Yin et al., 2023) for IGLUE; ColBERTv2 for MSMARCO (MM); FLMR (Lin et al.,\\n2023b) for Infoseek and OKVQA; and Google Lens (Google) for E-VQA. We follow the procedure as detailed in\\nthe Appendix C of the E-VQA paper (Mensink et al., 2023b) to use CLIP as a zero-shot retriever.\\nand #8 vs #5). Although we are only training on E-\\nVQA, the score on other KB-VQA tasks (Infoseek,\\nKVQA, OKVQA) increases by ∼1% or more. This\\nshows that E-VQA is an appropriate corpus for\\ntraining a general-purpose knowledge retriever. We\\nanalyze the gain from intermediate pre-training in\\nmore detail in Sec.5.6.\\n5.4 Ablation Studies\\nInstructions. Removing instructions (Table 2 #7)\\nresults in much worse overall performance, with\\nthe WIT recall rate reduced to 13.3. This shows that\\ninstructions are necessary for multi-task learning\\nand that our instruction scheme works well (the full\\nlist of instructions is given in Appendix A).\\nPre-training Datasets. As shown in Table 5,\\nadding CC3M to training improves performance\\non all metrics, showing that learning to understand\\nscene via captioning datasets is beneficial. Remov-\\ning either LLaV A or MSMARCO harms zero-shot\\nKB-VQA performance ( −3.0in Infoseek), noting\\n3The performance is not fully comparable due to differ-\\nences in the construction of the test passage corpus and the\\nproprietary nature of the data and pipeline used in Lens. The\\nreported figures serve as a reference point.Datasets WIT LLaV A Infoseek\\nAll 34.14 50.82 42.71\\nw/o CC3M 29.33 44.82 40.18\\nw/o LLaVA 33.78 30.78 39.20\\nw/o MSMARCO 33.96 47.88 38.90\\nw/o OVEN&KVQA 33.96 49.85 35.62\\nTable 5: Ablation study on Stage 1 pre-training datasets.\\nThe model is ViT-B + Base-v1. We evaluate systems\\non Infoseek in zero-shot mode though it is not used in\\nStage 1 training.\\nthat Infoseek is not used in this stage. Training on\\nthese datasets facilitates learning question-aware\\nvisual representations as the cross-attention in the\\nmapping structure must attend to the text input to\\nperform well on these tasks. Omitting knowledge-\\nintensive datasets (OVEN and KVQA) negatively\\nimpacts the zero-shot performance on Infoseek,\\nshowing the importance of in-domain data.\\nMapping Structure Scaling. Table 6 illustrates\\nthe impact of scaling up the mapping structure un-\\nder two PreFLMR configurations. Increasing cross-\\nattention layers from 1 to 4 marginally improves\\nLLaV A performance (+0.5, approx.), but adverselyModel MRR@10 Recall@50\\nSmall-v1 (28.8M) 34.5 79.8\\nMedium-v1 (41.4M) 35.5 81.4\\nBase-v1 (110M) 35.8 82.4\\nLarge-v1 (345M) 37.0 83.2\\nBase-v1 (official) 36.0 82.9\\nBase-v2 (official) 39.7 86.8\\nTable 3: Text encoder pre-training\\nresults evaluated on the full MS-\\nMARCO test set.Vis. Enc. Text Enc. WIT LLa. OVEN KVQA IGLUE Info. E-VQA OK. A.R.\\n1 ViT-B Base-v2 34.2 50.9 46.1 28.9 60.5 42.5 32.7 46.5 6.5\\n2 ViT-L Small-v1 46.5 46.1 37.9 17.9 57.3 43.5 26.6 56.7 7.0\\n3 ViT-L Medium-v1 49.6 47.8 38.6 23.1 58.7 46.7 27.7 58.1 5.3\\n4 ViT-L Base-v1 49.3 50.8 52.3 38.2 68.5 46.1 41.9 49.4 4.6\\n5 ViT-L Base-v2 49.6 51.2 54.8 40.5 69.5 48.7 45.0 50.9 2.3\\n6 ViT-L Large-v1 48.5 47.3 51.8 32.8 67.2 45.1 40.0 49.7 5.6\\n7 ViT-H Base-v2 51.8 51.6 55.3 35.6 69.0 48.6 42.2 51.3 2.8\\n8 ViT-G Base-v2 49.5 51.8 59.6 38.7 69.3 50.9 42.4 52.1 2.0\\nTable 4: PreFLMR performance after Stage 1. Infoseek, E-VQA, and OKVQA\\nare tested in zero-shot mode. A.R.: Average Rank against all other models on\\nall tasks. LLa.- LLaV A; Info.- Infoseek; OK. - OKVQA.\\nNTR WIT LLaV A Infoseek\\nViT-B + Base-v1 1L 34.1 50.8 42.7\\nViT-B + Base-v1 4L 29.0 51.4 40.8\\nViT-L + Base-v2 1L 49.6 51.2 48.7\\nViT-L + Base-v2 4L 45.9 51.7 46.8\\nTable 6: Performance of adding more Transformer lay-\\ners to the mapping structure. NTRis the number of\\nTransformer layers in the mapping structure.\\nimpacts performance on WIT (-4, approx.) and\\nInfoseek (-2, approx.). We adhere to the 1-layer de-\\nsign, noting that adding parameters to the mapping\\nstructure does not improve performance.\\n5.5 Retrieval Augmented Visual Question\\nAnswering with PreFLMR\\nModel OKVQA Infoseek E-VQA\\nBaseline 66.10 21.80 48.80\\nBaseline model PaLM-E PALI-X PaLM-B + Lens\\nA VIS 60.20 50.70/56.404-\\nRA-VQAv2 w/ FLMR 60.75 - -\\nRA-VQAv2 w/ PreFLMR 61.88 30.65 54.45\\nw/o retrieval 55.44 21.78 19.80\\nTable 7: Downstream KB-VQA performance when RA-\\nVQAv2 (Lin et al., 2023b) is equipped with PreFLMR\\nand fine-tuned on the target M2KR’s KB-VQA sub-\\ntasks. A VIS (Hu et al., 2024) is a recently published\\nhybrid system that leverages many planning stages to\\nsolve KB-VQA questions, which we include for refer-\\nence. Performance on Infoseek and E-VQA may not be\\ndirectly comparable to results in the literature.5\\nWe build on RA-VQAv2 (Lin et al., 2023b),\\na strong retrieval-augmented visual question an-\\nswering system to tackle OKVQA, Infoseek, and\\nE-VQA. We fine-tune the best-performing Pre-\\nFLMR variant on the target retrieval task (ViT-G +\\nBase-v2, Table 2 #14) and follow RA-VQAv2 to\\nfine-tune a BLIP-2 answer generator on the target\\n450.7 for Unseen Entity and 56.4 for Unseen Question; no\\noverall accuracy is reported.M2KR KB-VQA task.5Following previous liter-\\nature (Schwenk et al., 2022; Chen et al., 2023c;\\nMensink et al., 2023b), we use VQA score, Ac-\\ncuracy, and BERT matching (BEM) (Bulian et al.,\\n2022) to evaluate performance on OKVQA, Infos-\\neek, and E-VQA, respectively.\\nA brief summary of the systems shown in Ta-\\nble 7: PaLM-E (Driess et al., 2023), PALI-X (Chen\\net al., 2022) and PaLM-B (Anil et al., 2023)\\nare large multi-modal models with 562B, 55B,\\nand 1T parameters, respectively. The E-VQA\\nSOTA (Mensink et al., 2023b) uses Lens (Google),\\nthe Google API for image retrieval. A VIS (Hu\\net al., 2024) is a hybrid system with many compo-\\nnents (such as PaLI, PaLM, and Google Lens&Web\\nSearch API) and planning stages powered by LLMs.\\nWe note that PreFLMR could be used as part of\\nthe A VIS pipeline to enhance its ability to fetch\\nrelevant documents given questions and images.\\nAs shown in Table 7, compared to models with-\\nout retrieval, PreFLMR improves performance by\\napproximately 6% on OKVQA, 9% on Infoseek,\\nand 34% on E-VQA. These results highlight the\\neffectiveness of PreFLMR in document retrieval\\nfor KB-VQA tasks.\\nOn OKVQA, the performances of RA-VQAv2\\n(PreFLMR) and RA-VQAv2 (FLMR) are similar.\\nTable 2 #13 shows that PreFLMR attains similar Re-\\ncall@5 as FLMR on OKVQA even though it has a\\nmuch larger vision encoder. As a possible explana-\\ntion, compared to E-VQA and Infoseek, the knowl-\\nedge required to answer OKVQA question is less\\nspecialized and many OKVQA questions can be an-\\nswered without document retrieval (Mensink et al.,\\n2023b). See Appendix E for qualitative analysis.\\n5We note that this work was conducted during the early\\nstage of the release of Infoseek and E-VQA. We prepared the\\ndata splits according to the need for retrieval training following\\nAppendix A. The systems are trained and evaluated on the data\\nsplits provided in M2KR to show the improvement relative to\\nsystems without retrieval.Another possibility is that, compared to E-VQA\\nand Infoseek where the ground-truth document is\\nprovided for each question, the OKVQA training\\nset does not provide ground-truth knowledge doc-\\numents. The retriever uses pseudo-relevant doc-\\numents in training that contain the target answer\\nbut these may not be truly useful for answering the\\nquestion. This is evidence that data quality should\\nbe improved along with model scaling.\\n5.6 Analysis of Intermediate Pre-training\\nSec. 5.3 shows that Stage 2 Intermediate Pre-\\ntraining improves the performance as evaluated\\nby task-specific metrics. In this section, we further\\nquantify the gains from Stage 2 for each dataset\\nand more clearly show that KB-VQA tasks benefit\\nmore from Stage 2 than other tasks. We use the\\ndifference in minimal validation loss6achieved on\\neach dataset starting from checkpoints before or af-\\nter Stage 2 Intermediate Pre-training as a measure\\nof benefit. This enables comparison of tasks with\\ndifferent performance metrics. Intuitively, a larger\\nabsolute difference in validation loss indicates that\\nthe dataset benefits more from the Intermediate\\nPre-training stage.\\n0 5k 10k 15k 20k 25k\\nNinter0.10\\n0.08\\n0.06\\n0.04\\n0.02\\n0.00Val Loss (Ninter)\\n  Val Loss (Ninter=0)\\nBERT-medium\\n0 5k 10k 15k\\nNinterBERT-base\\nOven\\nEVQAKVQA\\nOKVQAInfoseek\\nMSMARCOWIT\\nLLaVASUM\\nFigure 2: Change in Stage 3 validation loss when ini-\\ntialized from Stage 2 checkpoints after Ninter steps of\\nintermediate pre-training. A large difference indicates a\\ngreater gain from intermediate pre-training.\\nFigure 2 plots the difference in validation loss of\\nevery dataset when the starting checkpoints have\\nundergone Ninter intermediate pre-training steps\\nusing either BERT-medium or BERT-base as the\\ntext encoder backbone. As expected, starting from\\nE-VQA-pre-trained checkpoints yields lower val-\\nidation loss in knowledge-intensive tasks such as\\nOKVQA, KVQA, and OVEN after the same num-\\nber (5,000) of fine-tuning steps. Performance on\\n6We find that the validation loss is predictive of the actual\\nperformance. A lower validation loss usually suggests a better\\nperformance in the tasks that we study.these datasets indeed sees more gain from Stage\\n2 training (Table 2, #5 v.s. #8). Figure 2 also\\nindicates the existence of an optimal Ninter, be-\\nyond which the model overfits to E-VQA, harm-\\ning performance on other datasets. The larger\\nPreFLMR model with BERT-base text encoder\\noverfits faster than PreFLMR with BERT-medium\\n(Ninter≈15,000versus Ninter≈10,000). We\\nuse V-Entropy (Xu et al., 2020) to formalize our\\nanalysis as an empirical measure of mutual infor-\\nmation between datasets in Appendix D.\\n5.7 Summary of Findings\\nWe summarise the results of our investigations into\\nscaling behaviour as follows:\\n•The text encoder size need not exceed that\\nof BERT-base (110M) to achieve competitive\\nmulti-modal retrieval performance (Sec.5.2).\\n•Scaling up the vision encoder from ViT-B to\\nViT-G yields substantial gains (Sec.5.2).\\n•Scaling up the mapping structure does not im-\\nprove performance (Sec.5.4).\\n•Intermediate pre-training on high-quality in-\\ndomain data (E-VQA) effectively improves\\nretrieval performance across KB-VQA tasks\\n(Sec.5.3, 5.6).\\n•Strong knowledge retrievers boost perfor-\\nmance on challenging KB-VQA tasks such as\\nOKVQA, Infoseek, and E-VQA via Retrieval-\\nAugmented Generation (Sec.5.5).\\n•Ground-truth document labels are important to\\nmake full use of large models in training multi-\\nmodal retrievers (Sec.5.5).\\n6 Conclusion\\nThis work has studied the scaling behaviour of\\nstate of the art multi-modal document retrieval sys-\\ntems, with a focus on enhancing fine-grained late-\\ninteraction retrieval for knowledge-based visual\\nquestion answering. We contribute a comprehen-\\nsive training and evaluation framework, M2KR, for\\ngeneral-purpose multi-modal knowledge retrieval.\\nThe PreFLMR system we train in the M2KR frame-\\nwork yields excellent retrieval performance across\\na range of tasks and can also serve as a base for\\nfurther task-specific fine-tuning.Limitations\\nLimited by available computational resources, we\\nleave several further investigations as future work:\\n(1) The CLIP-ViT models (Cherti et al., 2023) were\\nnot pre-trained on in-domain data of knowledge-\\nintensive tasks. Further training may enhance the\\nmodel’s ability to recognize a broader range of ob-\\njects; (2) Advanced training approaches beyond\\ncontrastive learning, such as score distillation (San-\\nthanam et al., 2022b), could be explored to further\\nenhance retrieval performance; (3) Investigating\\na more optimal mix proportion of datasets with\\nvarying sizes also warrants further exploration.\\nEthics Statement\\nOur proposed model retrieves documents without\\ngenerating new content. We acknowledge the po-\\ntential for the retrieved documents to include in-\\nappropriate information if the document database\\nlacks adequate filtering. Consequently, extra care\\nmust be taken to ensure the sanitization of the doc-\\nument database, particularly when employing this\\nmodel in applications involving direct interaction\\nwith real users.\\nAcknowledgments\\nThis work was supported in part by the AWS Cloud\\nCredit for Research programme.\\nWeizhe Lin is supported by a Research\\nStudentship funded by Toyota Motor Europe\\n(RG92562(24020)) for the undertaking of the PhD\\nin Engineering at the University of Cambridge.\\nJingbiao Mei is supported by Cambridge Com-\\nmonwealth, European and International Trust for\\nthe undertaking of the PhD in Engineering at the\\nUniversity of Cambridge.\\nJinghong Chen is supported by the Warwick\\nPostgraduate Studentship from Christ’s College\\nand the Huawei Hisilicon Studentship for the under-\\ntaking of the PhD in Engineering at the University\\nof Cambridge.\\nProf. Bill Byrne holds concurrent appointments\\nas a Professor of Information Engineering at Cam-\\nbridge University and as an Amazon Scholar. This\\npublication describes work performed at Cam-\\nbridge University and is not associated with Ama-\\nzon.\\nWe would also like to thank all the reviewers for\\ntheir knowledgeable reviews.References\\nIbrahim M Alabdulmohsin, Behnam Neyshabur, and\\nXiaohua Zhai. 2022. Revisiting neural scaling laws\\nin language and vision. In Advances in Neural Infor-\\nmation Processing Systems , volume 35, pages 22300–\\n22312. Curran Associates, Inc.\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\\nChen, Eric Chu, Jonathan H. Clark, Laurent El\\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\\nJan Botha, James Bradbury, Siddhartha Brahma,\\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha\\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\\nShelby, Ambrose Slone, Daniel Smilkov, David R.\\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\\nreport.\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh\\nTiwary, and Tong Wang. 2018. Ms marco: A human\\ngenerated machine reading comprehension dataset.\\n(arXiv:1611.09268). ArXiv:1611.09268 [cs].\\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\\n2014. Food-101 – mining discriminative components\\nwith random forests. In European Conference on\\nComputer Vision .\\nEmanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva\\nReddy, Desmond Elliott, Edoardo Maria Ponti, and\\nIvan Vuli ´c. 2022. Iglue: A benchmark for transferlearning across modalities, tasks, and languages. In\\nProceedings of the 39th International Conference on\\nMachine Learning , page 2370–2392. PMLR.\\nJannis Bulian, Christian Buck, Wojciech Gajewski, Ben-\\njamin Börschinger, and Tal Schuster. 2022. Tomayto,\\ntomahto. beyond token-level answer equivalence for\\nquestion answering evaluation. In Proceedings of the\\n2022 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 291–305, Abu Dhabi,\\nUnited Arab Emirates. Association for Computa-\\ntional Linguistics.\\nAndrea Burns, Krishna Srinivasan, Joshua Ainslie, Ge-\\noff Brown, Bryan A. Plummer, Kate Saenko, Jianmo\\nNi, and Mandy Guo. 2023. Wikiweb2m: A page-\\nlevel multimodal wikipedia dataset.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers) , pages 1870–1879,\\nVancouver, Canada. Association for Computational\\nLinguistics.\\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil\\nMustafa, Soravit Changpinyo, Jialin Wu, Car-\\nlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang,\\nYi Tay, et al. 2023a. Pali-x: On scaling up a multi-\\nlingual vision and language model. arXiv preprint\\narXiv:2305.18565 .\\nXi Chen, Xiao Wang, Soravit Changpinyo, A. J.\\nPiergiovanni, Piotr Padlewski, Daniel Salz, Sebas-\\ntian Goodman, Adam Grycner, Basil Mustafa, Lu-\\ncas Beyer, Alexander Kolesnikov, Joan Puigcerver,\\nNan Ding, Keran Rong, Hassan Akbari, Gaurav\\nMishra, Linting Xue, Ashish Thapliyal, James\\nBradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme,\\nAndreas Steiner, Anelia Angelova, Xiaohua Zhai,\\nNeil Houlsby, and Radu Soricut. 2022. Pali: A\\njointly-scaled multilingual language-image model.\\n(arXiv:2209.06794). ArXiv:2209.06794 [cs].\\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-\\nravit Changpinyo, Alan Ritter, and Ming-Wei Chang.\\n2023b. Can pre-trained vision and language models\\nanswer visual information-seeking questions? In\\nProceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing , pages\\n14948–14968, Singapore. Association for Compu-\\ntational Linguistics.\\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So-\\nravit Changpinyo, Alan Ritter, and Ming-Wei Chang.\\n2023c. Can pre-trained vision and language mod-\\nels answer visual information-seeking questions?\\n(arXiv:2302.11713). ArXiv:2302.11713 [cs].\\nZhuo Chen, Yufeng Huang, Jiaoyan Chen, Yuxia Geng,\\nYin Fang, Jeff Z. Pan, Ningyu Zhang, and Wen Zhang.\\n2023d. Lako: Knowledge-driven visual question\\nanswering via late knowledge-to-text injection. InProceedings of the 11th International Joint Confer-\\nence on Knowledge Graphs , IJCKG ’22, page 20–29,\\nNew York, NY , USA. Association for Computing\\nMachinery.\\nMehdi Cherti, Romain Beaumont, Ross Wightman,\\nMitchell Wortsman, Gabriel Ilharco, Cade Gordon,\\nChristoph Schuhmann, Ludwig Schmidt, and Jenia\\nJitsev. 2023. Reproducible scaling laws for con-\\ntrastive language-image learning. In 2023 IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition (CVPR) , pages 2818–2829.\\nYin Cui, Yang Song, Chen Sun, Andrew Howard, and\\nSerge Belongie. 2018. Large scale fine-grained cat-\\negorization and domain-specific transfer learning.\\n(arXiv:1806.06193). ArXiv:1806.06193 [cs].\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics . ACL.\\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\\nWahid, Jonathan Tompson, Quan Vuong, Tianhe\\nYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\\nmanet, Daniel Duckworth, Sergey Levine, Vincent\\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus\\nGreff, Andy Zeng, Igor Mordatch, and Pete Florence.\\n2023. Palm-e: An embodied multimodal language\\nmodel. In Proceedings of the 40th International Con-\\nference on Machine Learning , ICML’23. JMLR.org.\\nJulian Eisenschlos, Syrine Krichene, and Thomas\\nMüller. 2020. Understanding tables with interme-\\ndiate pre-training. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2020 , pages\\n281–296, Online. Association for Computational Lin-\\nguistics.\\nFeng Gao, Qing Ping, Govind Thattai, Aishwarya Re-\\nganti, Ying Nian Wu, and Prem Natarajan. 2022.\\nTransform-retrieve-generate: Natural language-\\ncentric outside-knowledge visual question answer-\\ning. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages\\n5067–5077.\\nFrançois Garderes, Maryam Ziaeefard, Baptiste Abe-\\nloos, and Freddy Lecue. 2020. Conceptbert:\\nConcept-aware representation for visual question an-\\nswering. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing:\\nFindings , pages 489–498.\\nGoogle. Google lens: Image recognition and retrieval\\napi.https://lens.google.com .\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\\nBatra, and Devi Parikh. 2017. Making the V in VQA\\nmatter: Elevating the role of image understanding\\nin Visual Question Answering. In Conference on\\nComputer Vision and Pattern Recognition (CVPR) .Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Haupt-\\nmann, Yonatan Bisk, and Jianfeng Gao. 2021. Kat:\\nA knowledge augmented transformer for vision-and-\\nlanguage. arXiv preprint arXiv:2112.08614 .\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Machine\\nLearning , ICML’20. JMLR.org.\\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\\nChen. 2022. LoRA: Low-rank adaptation of large\\nlanguage models. In International Conference on\\nLearning Representations .\\nHexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-\\nwal, Mandar Joshi, Kenton Lee, Kristina Toutanova,\\nand Ming-Wei Chang. 2023a. Open-domain vi-\\nsual entity recognition: Towards recognizing mil-\\nlions of wikipedia entities. (arXiv:2302.11154).\\nArXiv:2302.11154 [cs].\\nZiniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang,\\nYizhou Sun, David Ross, Cordelia Schmid, and\\nAlireza Fathi. 2024. Avis: Autonomous visual in-\\nformation seeking with large language model agent.\\nAdvances in Neural Information Processing Systems ,\\n36.\\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-\\nWei Chang, Yizhou Sun, Cordelia Schmid, David A.\\nRoss, and Alireza Fathi. 2023b. Reveal: Retrieval-\\naugmented visual-language pre-training with multi-\\nsource multimodal knowledge memory. page\\n23369–23379.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume ,\\npages 874–880, Online. Association for Computa-\\ntional Linguistics.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\\nBillion-scale similarity search with GPUs. IEEE\\nTransactions on Big Data , 7(3):535–547.\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\\nScaling laws for neural language models. arXiv\\npreprint arXiv:2001.08361 .\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for open-\\ndomain question answering. In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 6769–6781,\\nOnline. Association for Computational Linguistics.\\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\\ncient and effective passage search via contextualizedlate interaction over bert. In Proceedings of the 43rd\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval , SIGIR\\n’20, page 39–48, New York, NY , USA. Association\\nfor Computing Machinery.\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\\nmethod for stochastic optimization. In 3rd Inter-\\nnational Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings .\\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-\\nFei. 2013. 3d object representations for fine-grained\\ncategorization. In 4th International IEEE Workshop\\non 3D Representation and Recognition (3dRR-13) ,\\nSydney, Australia.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Computa-\\ntional Linguistics , pages 6086–6096, Florence, Italy.\\nAssociation for Computational Linguistics.\\nPaul Lerner, Olivier Ferret, and Camille Guinaudeau.\\n2023. Multimodal inverse cloze task for knowledge-\\nbased visual question answering. In European Con-\\nference on Information Retrieval , pages 569–587.\\nSpringer.\\nPaul Lerner, Olivier Ferret, and Camille Guinaudeau.\\n2024. Cross-modal retrieval for knowledge-based\\nvisual question answering. In European Conference\\non Information Retrieval , pages 421–438. Springer.\\nPaul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé\\nLe Borgne, Romaric Besançon, José G Moreno, and\\nJesús Lovón Melgarejo. 2022. Viquae, a dataset for\\nknowledge-based visual question answering about\\nnamed entities. In Proceedings of the 45th Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval , pages 3108–\\n3120.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems , 33:9459–9474.\\nGuohao Li, Xin Wang, and Wenwu Zhu. 2020. Boost-\\ning visual question answering with context-aware\\nknowledge aggregation. In Proceedings of the 28th\\nACM International Conference on Multimedia , pages\\n1227–1235.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\\n2023. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large lan-\\nguage models. arXiv preprint arXiv:2301.12597 .Leroy Lin, Yujia Xie, Dongdong Chen, Yichong Xu,\\nChenguang Zhu, and Lu Yuan. 2022. REVIVE: Re-\\ngional visual representation matters in knowledge-\\nbased visual question answering. In Advances in\\nNeural Information Processing Systems .\\nWeizhe Lin, Rexhina Blloshmi, Bill Byrne, Adria\\nde Gispert, and Gonzalo Iglesias. 2023a. LI-RAGE:\\nLate interaction retrieval augmented generation with\\nexplicit signals for open-domain table question an-\\nswering. In Proceedings of the 61st Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 2: Short Papers) , pages 1557–1566, Toronto,\\nCanada. Association for Computational Linguistics.\\nWeizhe Lin and Bill Byrne. 2022. Retrieval augmented\\nvisual question answering with outside knowledge.\\nInProceedings of the 2022 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n11238–11254, Abu Dhabi, United Arab Emirates. As-\\nsociation for Computational Linguistics.\\nWeizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru\\nCoca, and Bill Byrne. 2023b. Fine-grained late-\\ninteraction multi-modal retrieval for retrieval aug-\\nmented visual question answering. In Thirty-seventh\\nConference on Neural Information Processing Sys-\\ntems.\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\\nLee. 2023a. Improved baselines with visual instruc-\\ntion tuning. arXiv preprint arXiv:2310.03744 .\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and\\nYong Jae Lee. 2023b. Visual instruction tuning.\\n(arXiv:2304.08485). ArXiv:2304.08485 [cs].\\nMan Luo, Yankai Zeng, Pratyay Banerjee, and Chitta\\nBaral. 2021. Weakly-supervised visual-retriever-\\nreader for knowledge-based question answering. In\\nProceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing , pages\\n6417–6431, Online and Punta Cana, Dominican Re-\\npublic. Association for Computational Linguistics.\\nKenneth Marino, Xinlei Chen, Devi Parikh, Abhinav\\nGupta, and Marcus Rohrbach. 2021. Krisp: Inte-\\ngrating implicit and symbolic knowledge for open-\\ndomain knowledge-based vqa. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 14111–14121.\\nThomas Mensink, Jasper Uijlings, Lluis Castrejon,\\nArushi Goel, Felipe Cadar, Howard Zhou, Fei Sha,\\nAndré Araujo, and Vittorio Ferrari. 2023a. Encyclo-\\npedic vqa: Visual questions about detailed properties\\nof fine-grained categories. In Proceedings of the\\nIEEE/CVF International Conference on Computer\\nVision (ICCV) , pages 3113–3124.\\nThomas Mensink, Jasper Uijlings, Lluis Castrejon,\\nArushi Goel, Felipe Cadar, Howard Zhou, Fei Sha,\\nAndré Araujo, and Vittorio Ferrari. 2023b. Encyclo-\\npedic vqa: Visual questions about detailed proper-\\nties of fine-grained categories. (arXiv:2306.09224).\\nArXiv:2306.09224 [cs].Medhini Narasimhan, Svetlana Lazebnik, and Alexan-\\nder Schwing. 2018. Out of the box: Reasoning with\\ngraph convolution nets for factual visual question an-\\nswering. Advances in neural information processing\\nsystems , 31.\\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo\\nHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\\nLarge dual encoders are generalizable retrievers. In\\nProceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing , pages\\n9844–9855, Abu Dhabi, United Arab Emirates. As-\\nsociation for Computational Linguistics.\\nOpenAI. 2023. Gpt-4 technical report.\\nChen Qu, Hamed Zamani, Liu Yang, W Bruce Croft,\\nand Erik Learned-Miller. 2021. Passage retrieval for\\noutside-knowledge visual question answering. In\\nProceedings of the 44th International ACM SIGIR\\nConference on Research and Development in Infor-\\nmation Retrieval , pages 1753–1757.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\\ning transferable visual models from natural language\\nsupervision. arXiv .\\nJiahua Rao, Zifei Shan, Longpo Liu, Yao Zhou, and\\nYuedong Yang. 2023. Retrieval-based knowledge\\naugmented vision language pre-training. In Proceed-\\nings of the 31st ACM International Conference on\\nMultimedia , MM ’23, page 5399–5409, New York,\\nNY , USA. Association for Computing Machinery.\\nKeshav Santhanam, Omar Khattab, Christopher Potts,\\nand Matei Zaharia. 2022a. Plaid: An efficient en-\\ngine for late interaction retrieval. In Proceedings of\\nthe 31st ACM International Conference on Informa-\\ntion & Knowledge Management , CIKM ’22, page\\n1747–1756, New York, NY , USA. Association for\\nComputing Machinery.\\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\\nChristopher Potts, and Matei Zaharia. 2022b. Col-\\nBERTv2: Effective and efficient retrieval via\\nlightweight late interaction. In Proceedings of the\\n2022 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies , pages 3715–3734, Seat-\\ntle, United States. Association for Computational\\nLinguistics.\\nDustin Schwenk, Apoorv Khandelwal, Christopher\\nClark, Kenneth Marino, and Roozbeh Mottaghi.\\n2022. A-okvqa: A benchmark for visual question\\nanswering using world knowledge. arXiv preprint\\narXiv:2206.01718 .\\nSanket Shah, Anand Mishra, Naganand Yadati, and\\nPartha Pratim Talukdar. 2019. Kvqa: Knowledge-\\naware visual question answering. Proceedings\\nof the AAAI Conference on Artificial Intelligence ,\\n33(01):8876–8884.Piyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A cleaned,\\nhypernymed, image alt-text dataset for automatic im-\\nage captioning. In Proceedings of the 56th Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers) , page 2556–2565,\\nMelbourne, Australia. Association for Computational\\nLinguistics.\\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\\nMichael Bendersky, and Marc Najork. 2021. Wit:\\nWikipedia-based image text dataset for multimodal\\nmultilingual machine learning. In Proceedings of\\nthe 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval ,\\nSIGIR ’21, page 2443–2449, New York, NY , USA.\\nAssociation for Computing Machinery.\\nCong Wei, Yang Chen, Haonan Chen, Hexiang Hu,\\nGe Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.\\n2023. Uniir: Training and benchmarking universal\\nmultimodal information retrievers. arXiv preprint\\narXiv:2311.17136 .\\nTobias Weyand, Andre Araujo, Bingyi Cao, and Jack\\nSim. 2020. Google landmarks dataset v2 – a large-\\nscale benchmark for instance-level recognition and\\nretrieval. (arXiv:2004.01804). ArXiv:2004.01804\\n[cs].\\nJialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh\\nMottaghi. 2022. Multi-modal answer validation for\\nknowledge-based vqa. In Proceedings of the AAAI\\nConference on Artificial Intelligence , volume 36,\\npages 2712–2721.\\nJialin Wu and Raymond Mooney. 2022. Entity-focused\\ndense passage retrieval for outside-knowledge visual\\nquestion answering. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 8061–8072, Abu Dhabi, United\\nArab Emirates. Association for Computational Lin-\\nguistics.\\nYilun Xu, Shengjia Zhao, Jiaming Song, Russell Stew-\\nart, and Stefano Ermon. 2020. A theory of usable\\ninformation under computational constraints. In 8th\\nInternational Conference on Learning Representa-\\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\\n30, 2020 . OpenReview.net.\\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu,\\nMinzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo\\nLi, Xin Jiang, and Chunjing Xu. 2022. FILIP: Fine-\\ngrained interactive language-image pre-training. In\\nInternational Conference on Learning Representa-\\ntions .\\nDa Yin, Feng Gao, Govind Thattai, Michael Johnston,\\nand Kai-Wei Chang. 2023. Givl: Improving geo-\\ngraphical inclusivity of vision-language models with\\npre-training methods. page 10951–10961.\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\\nvision-language understanding with advanced large\\nlanguage models. arXiv preprint arXiv:2304.10592 .A Datasets details\\nThis section outlines the preprocessing methods\\nused to convert various datasets into formats suit-\\nable for retrieval tasks. Table 8 provides examples\\nfrom each dataset, demonstrating the transforma-\\ntion from their original to the adapted structure.\\nSubsequent subsections detail the specific prepro-\\ncessing steps for each dataset. The M2KR dataset\\nis available at Huggingface Hub.\\nA.1 I2T Retrieval\\nA.1.1 WIT\\nWIT (Srinivasan et al., 2021) is a corpus based on\\nWikipedia with image-text pairs, where the text\\nis the Wikipedia passage associated with the im-\\nage. To enhance data quality, we exclusively select\\nimage-text pairs where the images are the main/title\\nimages of their respective Wikipedia documents,\\nand we limit our scope to English-language docu-\\nments.\\nOur training set, comprising 2.8 million exam-\\nples, is sourced from the original WIT training set.\\n20,102 and 5,120 examples from the original WIT\\nvalidation set are selected to build the validation set\\nand test set in our M2KR benchmark, respectively.\\nThe test corpus includes all documents from the\\noriginal WIT validation and test sets. This setting\\nensures that there is no overlap between different\\nsets.\\nEach image-document pair is paired with a ran-\\ndomly selected instruction from our set of tem-\\nplates. The task is to retrieve the correct document\\nfrom the test corpus, given the image and instruc-\\ntion.\\nA.1.2 IGLUE\\nThe IGLUE English retrieval test set (Bugliarello\\net al., 2022), which is a subset of the WIT test\\nset and has an established benchmark for image-\\nto-text retrieval, is included to enable compari-\\nson with models in previous literature. Following\\nBugliarello et al. (2022), the test set contains 685\\nunique images and 1,000 Wikipedia passages. The\\ntask is similar to WIT: using the image and the\\ninstruction to retrieve the corresponding Wikipedia\\npassage.\\nInstruction templates for WIT and IGLUE :\\n•<Image> Identify the document that is con-\\nnected to this image.\\n•<Image> Provide information about the docu-\\nment linked to this image.•<Image> Please describe the document that\\ncorresponds to this image.\\n•<Image> What is the document that this image\\nis related to?\\n•<Image> Could you elucidate the document\\nassociated with this image?\\n•<Image> Describe the document that accompa-\\nnies this image.\\n•<Image> Please give information on the docu-\\nment that goes with this image.\\n•<Image> What document is represented by this\\nimage?\\n•<Image> Identify the document that this image\\npertains to.\\nA.1.3 KVQA\\nKVQA (Shah et al., 2019) is a dataset containing\\na rich collection of entities representing famous\\nindividuals. The KVQA task, initially designed as\\na KB-VQA task, has been re-purposed into an I2T\\ntask for our modelling purposes. This adaptation is\\nbased on our findings that using images as queries\\nalone suffices to retrieve the documents containing\\nthe correct identities. In our context, where the\\nprimary focus is on document retrieval, the origi-\\nnal questions are unnecessary. Our reformulated\\ntask for KVQA is to retrieve the details of famous\\npeople like gender, nationality, birthplace, and em-\\nployment history based solely on their images. The\\ntraining set is downsampled from the KVQA origi-\\nnal training set by removing repeated examples of\\nthe same famous individuals. We transformed the\\nstructured entities such as gender and nationality\\ninto passages. For example, “nationality: America;\\ndate of birth: dd/mm/yyyy; ...” is serialized as “na-\\ntionality is America, date of birth is dd/mm/yyyy,\\n...”.\\nThe training corpus is composed of all the doc-\\numents that appear in the original KVQA training\\nset. For the validation/test set, we selected a subset\\nof 13,365/5,120 samples from the original KVQA\\nvalidation set. Correspondingly, the test corpus\\nencompasses all documents found in the original\\nKVQA validation set.\\nThe instruction we use for KVQA is: <Image>\\nProvide a brief description of the image and the\\nrelevant details of the person in the image.\\nA.1.4 CC3M\\nCC3M (Sharma et al., 2018) is a dataset consisting\\nof a vast collection of image-caption pairs. Instead\\nof utilizing the entire dataset comprising 3 millionpairs, we adopt the downsampling methodology as\\ndelineated in LLaV A’s work (Liu et al., 2023b), re-\\nsulting in a reduced dataset of approximately 595K.\\nWe reformulate the image-caption pairs into\\nimage-to-text retrieval tasks in our pre-training. To\\nconstruct the training corpus, we treat each caption\\nas an individual document linked to its correspond-\\ning image. The task then involves retrieving the\\nmost relevant caption for a given image, guided\\nby a set of randomly selected instructions. Since\\nCC3M is originally an image captioning task, we\\ndo not validate or test our retriever on CC3M.\\nInstruction templates for CC3M\\n• <Image> Describe the image concisely.\\n•<Image> Provide a brief description of the\\ngiven image.\\n•<Image> Offer a succinct explanation of the\\npicture presented.\\n•<Image> Summarize the visual content of the\\nimage.\\n•<Image> Give a short and clear explanation of\\nthe subsequent image.\\n•<Image> Share a concise interpretation of the\\nimage provided.\\n• <Image> Present a compact description of the\\nphoto’s key features.\\n•<Image> Relay a brief, clear account of the\\npicture shown.\\n•<Image> Render a clear and concise summary\\nof the photo.\\n•<Image> Write a terse but informative sum-\\nmary of the picture.\\n•<Image> Create a compact narrative represent-\\ning the image presented.\\nA.2 Q2T Retrieval\\nA.2.1 MSMARCO\\nMSMARCO (Bajaj et al., 2018) stands for Mi-\\ncrosoft Machine Reading Comprehension dataset.\\nIt is a text-only dataset with around 1 million ques-\\ntions and 8 million passages. At stage 0, we train\\naccording to ColBERT-v1 by Khattab and Zaharia\\n(2020). For later stages, we downsample the dataset\\nto 400K questions to balance between the multi-\\nmodal tasks and unimodal tasks. For the training\\ncorpus, we still use the full 8 million passages. For\\ntesting, we select 6,980 and 5,120 samples from\\nthe original MSMARCO validation set and sample\\n400K passages to retrieve from and ensure the sub-\\nset contains all ground-truth passages.\\nInstruction templates for MSMARCO :•<Blank image> Retrieve the document that\\nanswers this question. <Questions>\\n•<Blank image> Find the document that is most\\nrelevant to the question. <Questions>\\n•<Blank image> Obtain the document that re-\\nsolves this query. <Questions>\\n•<Blank image> Acquire the document that elu-\\ncidates this question. <Questions>\\n•<Blank image> Choose the document most\\nrelevant to the query. <Questions>\\n•<Blank image> Identify the document most\\napplicable to the question. <Questions>\\n•<Blank image> Extract the document that an-\\nswers this query. <Questions>\\n•<Blank image> Locate the document that ad-\\ndresses the query.<Questions>\\nA.3 IQ2T Retrieval\\nA.3.1 LLaV A\\nThe LLaV A instruction following dataset contains\\nGPT-3.5 generated high-quality conversation about\\nan image between a human and an AI assistant.\\nThere are around 150K rounds of conversations.\\nWe took each conversation (each question from the\\nhuman and the answer from the AI assistant) as\\na separate sample. This results in a total of 356K\\nsamples. Since there are no original validation or\\ntest sets associated with the LLaV A, we manually\\nsplit the sample pool into 351K training examples\\nand 5,120 test examples.\\nThe task is reformulated to an Image&Question\\nto Text retrieval task. The training corpus and test\\ncorpus each contain the associated answers as pas-\\nsages to be retrieved by the image and question\\npairs. We use two types of instruction templates\\ndepending on the preciseness of the question:\\n•<Image> Provide a brief description of the im-\\nage along with the following question: <Question>\\n•<Image> Provide a concise explanation of the\\nimage along with the following question: <Ques-\\ntion>\\nA.3.2 OVEN\\nOVEN is a dataset targeting open-domain visual\\nentity recognition. The dataset consists of two\\nsplits: entity set and query set. The entity set\\nis derived from image classification datasets such\\nas INaturalist2017 (Cui et al., 2018), Food-101\\n(Bossard et al., 2014), Cars196 (Krause et al., 2013)\\nand Google Landmarks Dataset v2 (Weyand et al.,\\n2020). The query set is derived from VQA datasets\\nsuch as VQAv2 (Goyal et al., 2017) and OKVQA(Schwenk et al., 2022). To avoid overlapping with\\nour other KB-VQA datasets, we only use the entity\\nset of OVEN. The entity set contains about 10K\\nunique entities.\\nThe original entity set contains about 5 million\\nquestion-image pairs. However, the questions are\\nhighly duplicated in the original OVEN dataset.\\nWe downsample the dataset by removing repeated\\nquestions corresponding to the same entity. This\\nreduces duplications while maximizing the diver-\\nsity of the questions and coverage of entities. After\\nthe filtering, we keep 339K training samples. For\\nvalidation and testing, we select 20,000 and 5,120\\nexamples from the original OVEN Entity valida-\\ntion set. The original test set is not used in M2KR\\ndue to the lack of annotation.\\nThe original task is to link the image to a specific\\nWikipedia Entity given a question. To formulate the\\ntask as a retrieval problem, for each entity, we use\\nits associated Wikipedia passage as the document\\nto retrieve. The query side of this retrieval task\\ncontains the image and its question with the inclu-\\nsion of a randomly sampled instruction. Given this\\nquery, the task is to obtain the relevant Wikipedia\\npassage. The training corpus contains about 10K\\npassages, while the test corpus contains about 3.2K\\npassages that cover all entities in OVEN’s original\\ntraining set and validation set respectively.\\nA.3.3 E-VQA, Infoseek and OKVQA\\nE-VQA, Infoseek, and OKVQA are Knowledge-\\nbased VQA (KB-VQA) datasets. For each given\\nimage and question (with instruction), the task is\\nto retrieve the corresponding knowledge passage.\\nFor E-VQA (Mensink et al., 2023a), the orig-\\ninal training set contains around 1 million sam-\\nples. However, it includes duplicated questions\\nand answers referring to the same Wikipedia Entity\\nwith different query images. We filter duplicated\\nquestions that pertain to the same Wikipedia Entity.\\nTo align with the original evaluation setting of E-\\nVQA, we further excluded samples that necessitate\\nmultiple knowledge bases, reducing the count to\\n167K training samples. To be consistent with the\\noriginal E-VQA paper, our validation and testing\\nsets exclusively include questions that can be an-\\nswered using single knowledge. These sets contain\\n9,852 and 3,750 samples, respectively. We use the\\nWikiWeb2M (Burns et al., 2023) as the knowledge\\nsource. For the training and test passage corpus,\\nwe keep all the passages that appear in the original\\nE-VQA to align with the official E-VQA’s settingfor retrieval.\\nForOKVQA , we use the original training and\\ntest set. Following Lin et al. (2023b), we prepare\\na knowledge corpus with Wikipedia documents\\nbased on pseudo-relevance. The training and test\\npassage corpus both contain all passages in the\\nknowledge corpus.\\nForInfoseek , following the preprocessing steps\\ndescribed by Chen et al. 2023c, we use Wikipedia\\ndocuments as knowledge sources and remove ex-\\namples whose answers can not be found in the\\nground-truth documents. We randomly selected\\n100K examples from the training set for training\\nand 4,708 examples from the validation set for test-\\ning (the annotation of the original test set has yet\\nbeen released). The downsampling is motivated by\\nour observation that many questions are repeated\\nand the number of unique documents associated\\nwith the whole dataset is only about 40K. We down-\\nsampled the dataset such that the model won’t over-\\nfit severely to Infoseek passages.\\nNote that the aforementioned downsampling pro-\\ncedure for the test set is only used for constructing\\nthe M2KR benchmark. For downstream VQA eval-\\nuation, we use the same test set that existed in\\nprevious literature to ensure a fair comparison.\\nInstruction templates for OVEN, Infoseek, E-\\nVQA, and OKVQA\\n•<Image> Using the provided image, obtain\\ndocuments that address the subsequent question:\\n<Question>\\n•<Image> Retrieve documents that provide an\\nanswer to the question alongside the image: <Ques-\\ntion>\\n•<Image> Extract documents linked to the\\nquestion provided in conjunction with the image:\\n<Question>\\n•<Image> Utilizing the given image, obtain\\ndocuments that respond to the following question:\\n<Question>\\n•<Image> Using the given image, access docu-\\nments that provide insights into the following ques-\\ntion: <Question>\\n•<Image> Obtain documents that correspond to\\nthe inquiry alongside the provided image: <Ques-\\ntion>\\n•<Image> With the provided image, gather doc-\\numents that offer a solution to the question: <Ques-\\ntion>\\n•<Image> Utilizing the given image, obtain\\ndocuments that respond to the following question:\\n<Question>B Implementation Details\\nB.1 Breakdown of Data Used in Training\\nIn Stage 3 Full-scale Fine-tuning, the different sub-\\ntasks in the M2KR dataset are downsampled or\\nduplicated to balance the dataset proportions dur-\\ning training. The detailed breakdown of the data\\nused in different phases is presented in Table 9. We\\nobserved that without adjusting the data propor-\\ntions during training, the model’s training losses\\non certain datasets like WIT, Infoseek, and OVEN\\ndecrease much faster than on others once all pa-\\nrameters become trainable. This goes against our\\ngoal of training a multi-tasking system. Adjusting\\nthe data proportions is crucial to ensure a more\\nconsistent learning process across different tasks.\\nB.2 Detailed Hyperparameters\\nWe use the Adam optimizer (Kingma and Ba, 2015)\\nwith a fixed learning rate of 10−4for the mapping\\nstructure and 10−5for the rest parameters in all\\nexperiments in all training stages. 4 Nvidia A100\\nGPUs were used with data parallel in all experi-\\nments.\\nStage 0: Training was run up to 300k steps. The\\nbatch size is 8 and the gradient accumulation step\\nis 8. The number of negative examples is 1. The\\nvalidation ran per 10k steps. The checkpoint was\\ntaken at the best Recall@50 on the original MS-\\nMARCO validation set, following Khattab and Za-\\nharia (2020). The total training time is approxi-\\nmately 1.5 days per model.\\nStage 1: Training was run up to 220k. The batch\\nsize is 8 and the gradient accumulation step is 8.\\nThe number of negative examples is 4. The valida-\\ntion interval is 10k steps. The checkpoint was taken\\nat the best Recall@10 on the validation set of WIT\\nin M2KR. The total training time is approximately\\n5 days per model.\\nStage 2: The intermediate pre-training was run\\nfor 12k steps for all experiments. The batch size is\\n8 and the gradient accumulation step is 8. The num-\\nber of negative examples is 4. The total training\\ntime is approximately 2 days per model.\\nStage 3: Training was run for 50k for all exper-\\niments. Training was early-stopped if the perfor-\\nmance on WIT or E-VQA decreases for 3 consec-\\nutive validation runs. Validation was run per 10k\\nsteps. The batch size is 8 and the gradient accumu-\\nlation step is 8. The number of negative examples\\nis 4. The total training time is approximately 2\\ndays per model.WIT IGLUE KVQA CC3M MSMARCO\\nDescribe the image\\nconcisely.Summarize the visual\\ncontent of the image.Provide a brief de-\\nscription of the image\\nand the relevant de-\\ntails of the person in\\nthe image.Describe the image\\nconcisely.Retrieve the document\\nthat answers this ques-\\ntion: how many years\\ndid william bradford\\nserve as governor of\\nplymouth colony?\\ntitle: PS Herald sec-\\ntion title: Formation\\nand operation of the\\nNorth Shore Steam\\nCompany ...title: National Library\\nof Uzbekistan hier-\\narchical section title:\\nNational Library of\\nUzbekistan caption ...This is an image of\\nPilkington playing for\\nCardiff City in 2016.\\nAnthony Pilkington\\ndate of birth is ...olive oil is a healthy\\ningredient used liber-\\nally.William Bradford\\n(c.1590 - 1657) was\\nan English Separatist\\nleader in Leiden, ...\\nLLaV A OVEN E-VQA Infoseek OKVQA\\nProvide a brief de-\\nscription of the im-\\nage along with the fol-\\nlowing question: what\\nunique situation is oc-\\ncurring in this soccer\\nmatch?Using the provided im-\\nage, obtain documents\\nthat address the subse-\\nquent question: what\\nis this park called?Obtain documents that\\ncorrespond to the in-\\nquiry alongside the\\nprovided image: how\\nbig can this plant be-\\ncome?With the provided im-\\nage, gather documents\\nthat offer a solution to\\nthe question: What is\\nthe country of origin\\nof this food?Using the provided im-\\nage, obtain documents\\nthat address the subse-\\nquent question: How\\nmany teeth does this\\nanimal use to have?\\nIn this soccer match,\\na unique situation\\nis occurring where\\nthree men are playing\\nagainst each other,\\neach wearing a differ-\\nent colored uniform.Nationals Park is\\na baseball stadium\\nalong the Anacostia\\nRiver in the Navy\\nYard neighborhood...Dwarf cornel is a rhi-\\nzomatous herbaceous\\nperennial growing to\\n20cm (8 inches) tall...title: Submarine sand-\\nwich content: Subma-\\nrine sandwich A sub-\\nmarine sandwich, also\\nknown as a sub...Most cats have 26 de-\\nciduous teeth and 30\\npermanent teeth.\\nTable 8: Demonstration of the retrieval tasks for each dataset. We show the image (first row) query, the text query\\n(second row), and the retrieved ground truth document (third row) for each dataset. Since some retrieved documents\\nare too long, we only show part of the document and use ... to stand for continuing documents.Stage 1 Stage 2 Stage 3\\nWIT 2.8M - 140K\\nIGLUE - - -\\nKVQA 65K - 6.5K\\nCC3M 595K - 29.8K\\nMSMARCO 400K - 40K\\nOVEN 339K - 33.9K\\nLLaV A 351K - 35.1K\\nOKVQA 9K - 90K (repeat 10 times)\\nInfoseek 100K - 50K\\nE-VQA 167K 167K 167K\\nTable 9: The dataset sizes are adjusted in Stage 3 in\\npractice.\\nSingle-task Downstream Fine-tuning: The batch\\nsize is 8 and the gradient accumulation step is 8.\\nThe number of negative examples is 4. For ref-\\nerence, in our experiments, the downstream fine-\\ntuning took 20k, 5k, 1k, 15k, 2.5k steps to achieve\\nthe best performance for WIT, OVEN, Infoseek,\\nE-VQA, OKVQA respectively (for the ViT-G +\\nBase-v2 PreFLMR). The total training time is ap-\\nproximately 1 days per model per task.\\nVQA Fine-tuning: We used BLIP2-T5XL as\\nthe answer generator as in RA-VQAv2 (Lin et al.,\\n2023b). The retriever was frozen during training\\nand inference. The batch size is 1 and the gradient\\naccumulation step is 16. For each question in a\\ntraining batch, top-5 relevant documents were pre-\\nextracted using the retriever, and 3 out of 5 were\\nrandomly selected. These 3 documents were con-\\ncatenated to the question and sent to the answer\\ngenerator for one forward pass individually. This\\nsetting is to enable training with top-5 documents\\ngiven limited GPU memory. The total training time\\nis approximately 2 days per model.\\nEvery model reported in this paper was repro-\\nduced once to make sure the training is repro-\\nducible. The best result is reported since the model\\nwith the best result will be released to the com-\\nmunity. There is not much difference in the two\\nruns. The absolute difference is less than 0.2 Recall\\nscore in most datasets (except that PreFLMR_ViT-\\nB_Base-v2 has a -0.4 difference on Infoseek).\\nB.3 Large-v1 Training\\nIn our experiments, we found that training Large-v1\\nduring Stage 2/3 was not steady. First, the loss de-\\ncreased faster than in other systems, like Base-v1,\\neven though Large-v1 had worse system perfor-\\nmance. This happened because Large-v1’s bigger\\nmodel capacity made it more prone to overfitting.\\nNext, the loss suddenly shot up, causing the train-ing to collapse, despite using the same data and\\nstrategy as Base-v1. We tried different hyperpa-\\nrameters, like lowering the learning rate to 1e−6,\\n3e−6, but the model still collapsed.\\nFinally, when we used LoRA (Hu et al., 2022)\\nwith Large-v1 during training, it helped stabilize\\nthe process. The LoRA hyperparameters used were:\\nr= 16 ,α= 32 , and a dropout rate of 0.05.\\nB.4 Model Design in Detail\\nSimilar to FLMR, PreFLMR consists of three com-\\nponents: a vision model FV, a mapping structure\\nFM, and a language model FL.\\nFeature Extraction. The textual query qconsists\\nof an instruction and (optionally) a question (e.g.,\\n\"Utilize the given image to procure documents ad-\\ndressing the following query: [Question]\"). We use\\na language model with hidden size dLto obtain em-\\nbeddings for all Nqtokens which are concatenated\\ninto matrix Qq:\\nQq=FL(q)∈ RNq×dL (2)\\nLike FLMR, a vision model FVencodes the\\ninput image I, extracting the [CLS] token embed-\\ndings from the last layer. PreFLMR additionally\\nuses the patch embeddings from the penultimate\\nlayer of ViT for more complete representation.\\nQI,[CLS]=FV(I)∈ R1×dV (3)\\nQI,PATCH =FV,−2(I)∈ RNV×dV (4)\\nThe mapping structure FMcomprises two com-\\nponents: a 2-layer MLP FMLP\\nM and a Transformer\\nblockFTR\\nM.\\nFollowing the FLMR model, a 2-layer Multi-\\nLayer Perceptron (MLP) FMLP\\nM is utilized to con-\\nvert the initial token embeddings into visual token\\nembeddings with a length of Nvtand a hidden size\\ndh:7\\nQMLP\\nI =FMLP\\nM(QI,[CLS])∈ RNvt×dh(5)\\nMoreover, an additional Transformer module\\nFTR\\nMis introduced to manage all patch embeddings.\\nIt is a stack of NTRtransformer layers with a hid-\\nden size dL, followed by a simple MLP layer at the\\n7Transformation sequence: RdV→ RNvtdh/2→\\nRNvtdh, subsequently reshaped into RNvt×dh.end. This module leverages cross-attention with the\\ntext query Qq, enabling query-aware image feature\\nmapping.\\nQTR\\nI=FTR\\nM(Fv(QI,PATCH ),Qq)∈ RNV×dh\\n(6)\\nHere,Fvrepresents a 1-layer MLP that adapts\\nthe dimension from dVtodL, which is subse-\\nquently transformed to dhby the linear MLP layer\\nofFTR\\nM. The resultant features from these pro-\\ncesses are concatenated to formulate the query em-\\nbeddings:\\nQ=\\x02\\nQq|QMLP\\nI|QTR\\nI\\x03\\n∈ R(Nvt+NV+Nq)×dh\\n(7)\\nFurthermore, the document representations in\\nthe knowledge base are denoted by D, derived from\\nthe document content dwith length lD:\\nD=Fl(FL(d))∈ RlD×dh, (8)\\nwhere Flsignifies a straightforward MLP layer\\ntasked with mapping dLtodh, thereby aligning the\\ndimensionality with the query embeddings.\\nMulti-Modal Late Interaction. The relevance\\nscore between a question-image pair ¯q= (q, I)\\nand a document dis calculated using a late-\\ninteraction paradigm:\\nr(¯q, d) =r((q, I), d) =lQX\\ni=1lDmax\\nj=1QiD⊤\\nj (9)\\nwhere lQ=Nvt+NV+Nq. For each token\\nin the query, the system aggregates the maximum\\nrelevance score across all tokens in the document.\\nTraining and Inference. For model training,\\ndocuments d∗corresponding to a query qare con-\\nsidered gold (positive) samples. We incorporate\\nrandom negative sampling from the corpus.8Ad-\\nditionally, we adopt in-batch negative sampling as\\nsuggested by Karpukhin et al. (2020), treating all\\nnon-corresponding documents in a batch as nega-\\ntives for q, denoted as N(q). The model is trained\\nusing a contrastive loss across the dataset D:\\nL=−X\\n(q,d∗)∈Dlogexp (r(q, d∗))\\nexp (r(q, d∗)) +X\\nz∈N(q)exp (r(q, z))\\n(10)\\n8In multi-dataset scenarios, negative samples are selected\\nfrom the same corpus as d∗.Post-training, all documents are indexed through\\nPLAID (Santhanam et al., 2022a) for efficient late-\\ninteraction retrieval. For detailed evaluation of\\nretrieval efficiency, we refer readers to Lin et al.\\n(2023b).\\nC Ablation Study on Pre-training Stages\\nWe present the ablation study for the four pre-\\ntraining stages in Table 10. To ensure consistent\\ncomparison, these ablated versions underwent the\\nsame number of training steps as PreFLMR_ViT-\\nB_Base-v2. The results clearly indicate that the\\nremoval of any stage deteriorates performance.\\nSpecifically, disabling Stage 0 (i.e. using untrained\\ntext encoder) leads to the most significant per-\\nformance decline because the text encoder is not\\npre-trained on late-interaction, resulting in a di-\\nminished ability to capture fine-grained relevance\\nwithin the same computational budget. Note that\\nremoving Stage 0 leads to collapsed performance\\non Stage 1, where the text encoder is frozen. Fur-\\nthermore, removing Stage 2 notably affects the\\nperformance on E-VQA more than on other KB-\\nVQA datasets, highlighting the challenge posed\\nby E-VQA and the necessity of intermediate pre-\\ntraining.\\nD V-Entropy-based Analysis of\\nIntermediate Pre-training\\nV-Entropy (Xu et al., 2020), HV(Y|X), is the min-\\nimal Negative Log-Likelihood (NLL) achievable\\nby the probabilistic predictor f(Y|X)under the\\npredicative family V. A predicative family can\\nbe viewed as the set of reachable models under a\\ncertain model architecture and training budgets.\\nWe define Mutual Information IV[Nf](D1→\\nD2)between datasets D1andD2in Eq.11. We de-\\nfineHV[Nf](D2)as the minimal achieved NLL loss\\non the validation set of dataset D2afterNftrain-\\ning steps on D2.V[Nf, D1, Nt]denotes the set of\\nreachable models after Nffine-tuning steps on D2\\nstarting from a checkpoint that has been trained on\\ndataset D1forNtsteps. This is V-Entropy with\\nadditional predictive family specification.\\nIV[Nf](D1→D2) =HV[Nf](D2)\\n−HV[Nf,D1,Nt](D2)(11)\\nIntuitively, D1has high mutual information with\\nD2if models initialized from D1checkpoints at-\\ntain much lower NLL loss compared to modelsModel WIT IGLUE KVQA MM OVEN LLaV A Infoseek E-VQA OKVQA\\nPreFLMR_ViT-B_Base-v1 41.7 57.3 28.6 79.5 46.3 67.2 48.8 67.9 66.1\\nw/o Stage 0 25.5 28.8 21.0 56.5 33.9 55.0 42.5 51.8 64.5\\nw/o Stage 1 38.2 54.9 26.6 78.0 45.5 62.8 44.6 61.9 65.5\\nw/o Stage 2 41.2 56.8 26.5 78.2 43.7 65.0 47.0 57.3 65.1\\nTable 10: Retrieval performance when disabling pre-training stages. Removal of any stage deteriorated the\\nperformance.\\ninitialized without training on D1.NfandNtset\\nthe computation constraints for training on D2and\\nD1, respectively. In our experiment, Vis the Pre-\\nFLMR architecture, D1is the E-VQA dataset and\\nD2is the training set of M2KR. Nfcorresponds to\\nNinter in Sec. 5.6, which is the intermediate train-\\ning steps on the E-VQA dataset. In the analysis,\\nwe set Nfto 5,000 and sweep Ntfrom 0 to 25,000\\nin intervals of 5,000.\\nWe refer readers to Xu et al. (2020) for de-\\ntailed properties of V-Entropy and emphasize that\\nIV[Nf](D1→D2)is an empirical value we define\\nto estimate mutual information between datasets.\\nIt is different from the V-Information defined in Xu\\net al. (2020) which estimates the mutual informa-\\ntion between model input and output.\\nE Qualitative Analysis for OKVQA and\\nE-VQA\\nIn this section, we compare examples from the\\nOKVQA and E-VQA datasets to highlight their\\ndifferences. To avoid cherry-picking, we use exam-\\nples from its official website9for OKVQA. Sim-\\nilarly, we use the examples included in the paper\\nfor E-VQA. Table 11 presents three examples from\\neach dataset.\\nThe OKVQA examples typically require com-\\nmon sense knowledge, like ‘people attend church\\non Sundays’ or ‘firetrucks use fire hydrants.’ State-\\nof-the-art Large Language Models (LLMs) often\\nhave this common sense knowledge inherently\\nbuilt-in, making additional knowledge retrieval less\\nimpactful for OKVQA tasks.\\nIn contrast, E-VQA examples demand more spe-\\ncialized, expert-level knowledge, necessitating an\\neffective knowledge retrieval system. For instance,\\ncorrectly answering a question about ’Acacia para-\\ndoxa’ requires first retrieving the relevant doc-\\nument providing specific information about this\\nplant species. Enhancing the knowledge retrieval\\n9https://okvqa.allenai.org/system to source accurate documents is crucial for\\nimproving performance on the E-VQA dataset.\\nF Artifacts and License\\nWe list the resources used and their License below:\\n(1) huggingface-transformers (Apache Li-\\ncense 2.0) provides pre-trained model check-\\npoints for BLIP 2, DPR, and their tokenizers:\\nhttps://github.com/huggingface/transformers\\n(2) FAISS (Johnson et al., 2019) (MIT\\nLicense) is used to index document em-\\nbeddings for fast retrieval with DPR:\\nhttps://github.com/facebookresearch/faiss\\n(3) huggingface-PEFT (Apache License\\n2.0) for parameter-efficient LoRA fine-tuning:\\nhttps://github.com/huggingface/peft\\n(4) PLAID and ColBERTv2 (MIT License):\\nhttps://github.com/stanford-futuredata/ColBERT\\n(5) RA-VQA-v2 official reposi-\\ntory with training and testing codes\\n(GNU General Public License v3.0):\\nhttps://github.com/LinWeizheDragon/Retrieval-\\nAugmented-Visual-Question-Answering.\\n(6) Datasets used in building the M2KR bench-\\nmark:\\n•WIT (Creative Commons\\nAttribution-ShareAlike 3.0 Unported\\nhttps://github.com/google-research-\\ndatasets/wit/blob/main/LICENSE);\\n•MSMARCO (non-\\ncommercial research purposes\\nonly https://microsoft.github.io/msmarco/);\\n•CC3M (Free for any pur-\\nposes https://github.com/google-research-\\ndatasets/conceptual-captions);\\n•LLaV A, the image of LLaV A is a subset of\\nCC3M. It should inherit the license of CC3M.\\nThe conversation data follows policy of Ope-\\nnAI: https://openai.com/policies/terms-of-use.\\n•IGLUE (MIT license https://github.com/e-\\nbug/iglue/blob/main/LICENSE);OKVQA\\nQ: What days might I most commonly\\ngo to this building?Q: What sort of vehicle uses this item? Q: Is this photo from the 50’s or the\\n90’s?\\nA: Sunday A: firetruck A: 50’s\\nE-VQA\\nQ: How many feet tall does this tree\\ngrow to?Q: How many eggs does this reptile typ-\\nically lay?Q: Who founded this monastery?\\nA: 7 to 13 A: 3-6 A: Prince Constantin Brâncoveanu\\nTable 11: Demonstrative examples from OKVQA and E-VQA. Questions in E-VQA require more domain knowledge\\nto answer generally.\\n•KVQA (No specific license is men-\\ntioned https://malllabiisc.github.io/resources/kvqa/);\\n•OVEN (Apache-2.0 license\\nhttps://github.com/open-vision-\\nlanguage/oven/blob/main/LICENSE);\\n•E-VQA (no specific license mentioned\\nhttps://github.com/google-research/google-\\nresearch/tree/master/encyclopedic_vqa);\\n•Infoseek (Apache License 2.0\\nhttps://github.com/open-vision-\\nlanguage/infoseek/blob/main/LICENSE)\\n•OKVQA (Copyright (c) 2021, Chen Qu\\nand Center for Intelligent Information\\nRetrieval, University of Massachusetts,\\nAmherst. https://github.com/prdwb/okvqa-\\nrelease/blob/main/LICENSE)\\nIn particular, we emphasize that no changes are\\nmade to the original data of all the datasets used in\\nour work. Our released models and artifacts should\\nonly be used for non-commercial purposes. By\\nusing the pre-trained models, users agree to respect\\nthe terms and conditions of the datasets used in\\npre-training.G PreFLMR model performance radar\\nchart on M2KR tasks\\nFig. 3 demonstrates the performance of PreFLMR\\nwith a radar plot. The best and worst numbers of\\neach task are annotated.\\nH AI Assistance\\nOur coding work was assisted by Github Copilot.10\\nOpenAI ChatGPT11was only used in proofread-\\ning and spell-checking. We claim that the content\\npresented in this paper was fully original.\\n10https://github.com/features/copilot\\n11https://chat.openai.com/WITLLaVAOven\\nKVQA\\nMSMARCO\\nIGLUE\\nInfoseek\\nEVQAOKVQA\\n23.8 61.4656.472.6\\n40.563.42\\n28.6343.61\\n78.5486.9\\n30.8\\n71.5147.08\\n59.5662.5\\n73.0566.09\\n68.64\\nBaselines\\nViT-B Base-v2\\nViT-L Base-v2\\nViT-H Base-v2\\nViT-G Base-v2Figure 3: PreFLMR achieves strong performance on the M2KR benchmark. The scale of the plot is adjusted for\\nbetter visualization. The best and worst numbers of each task are annotated.', title=None, url_or_path='./docs/2402.08327v2.pdf')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SimplerLLM.tools.generic_loader import load_content\n",
    "\n",
    "# Load from various sources\n",
    "pdf_content = load_content(\"./docs/\"+pdf_paths[0])\n",
    "\n",
    "pdf_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e500d-1d85-44f1-b45c-7de5e190617c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
